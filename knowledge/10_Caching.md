# Domain 10: Caching
> **S·ªë practices:** 8 | üî¥ 2 | üü† 3 | üü° 3
> **Tr·ªçng s·ªë:** √ó1

---

## 10.01 - @Cacheable / @CacheEvict cho read-heavy data üü†

### Metadata
- **ID:** `CACHE-001`
- **M·ª©c ƒë·ªô:** üü† KHUY·∫æN NGH·ªä
- **L√Ω do:** Gi·∫£m 70-90% database queries, tƒÉng throughput, gi·∫£m latency
- **Trade-off:** Stale data risk, memory overhead, complexity tƒÉng

### T·∫°i sao?

**V·∫•n ƒë·ªÅ:**
- Query database m·ªói request ‚Üí latency cao (50-200ms/query)
- Read-heavy APIs (product catalog, user profile) g√¢y database bottleneck
- Scaling database ƒë·∫Øt h∆°n scaling cache (Redis) r·∫•t nhi·ªÅu

**Gi·∫£i ph√°p:**
- `@Cacheable` t·ª± ƒë·ªông cache k·∫øt qu·∫£ method
- `@CacheEvict` x√≥a cache khi data thay ƒë·ªïi
- `@CachePut` update cache m√† kh√¥ng skip method execution

**Khi n√†o d√πng:**
- ‚úÖ Data √≠t thay ƒë·ªïi (categories, settings, product details)
- ‚úÖ Read:Write ratio > 10:1
- ‚úÖ Query ph·ª©c t·∫°p (joins, aggregations)
- ‚ùå Real-time data (stock prices, live chat)
- ‚ùå Personalized data (cart, recommendations) ‚Üí d√πng session cache

### ‚úÖ C√°ch ƒë√∫ng

```java
// ===== Config: CacheConfig.java =====
@Configuration
@EnableCaching
public class CacheConfig {

  @Bean
  public CacheManager cacheManager(RedisConnectionFactory redisConnectionFactory) {
    // TTL cho t·ª´ng cache ri√™ng bi·ªát
    Map<String, RedisCacheConfiguration> cacheConfigs = Map.of(
      "products", RedisCacheConfiguration.defaultCacheConfig()
        .entryTtl(Duration.ofHours(1))
        .serializeValuesWith(RedisSerializationContext.SerializationPair
          .fromSerializer(new GenericJackson2JsonRedisSerializer())),

      "categories", RedisCacheConfiguration.defaultCacheConfig()
        .entryTtl(Duration.ofDays(1)), // Categories √≠t thay ƒë·ªïi

      "userProfiles", RedisCacheConfiguration.defaultCacheConfig()
        .entryTtl(Duration.ofMinutes(15)) // User data ƒë·ªïi th∆∞·ªùng xuy√™n h∆°n
    );

    return RedisCacheManager.builder(redisConnectionFactory)
      .cacheDefaults(RedisCacheConfiguration.defaultCacheConfig()
        .entryTtl(Duration.ofMinutes(10))) // Default TTL
      .withInitialCacheConfigurations(cacheConfigs)
      .build();
  }
}

// ===== Service: ProductService.java =====
@Service
@Slf4j
public class ProductService {

  @Autowired
  private ProductRepository productRepository;

  // Cache v·ªõi key = productId
  @Cacheable(value = "products", key = "#productId", unless = "#result == null")
  public ProductDto getProduct(Long productId) {
    log.info("Cache MISS - Querying DB for product: {}", productId);
    return productRepository.findById(productId)
      .map(this::toDto)
      .orElse(null);
  }

  // Cache v·ªõi composite key
  @Cacheable(
    value = "products",
    key = "#categoryId + ':' + #page + ':' + #size",
    condition = "#page < 10" // Ch·ªâ cache 10 trang ƒë·∫ßu
  )
  public Page<ProductDto> getProductsByCategory(
    Long categoryId,
    int page,
    int size
  ) {
    log.info("Cache MISS - Querying products for category: {}", categoryId);
    Pageable pageable = PageRequest.of(page, size);
    return productRepository.findByCategoryId(categoryId, pageable)
      .map(this::toDto);
  }

  // Cache v·ªõi SpEL expression
  @Cacheable(
    value = "products",
    key = "T(String).format('%s:%s', #filter.brand, #filter.priceRange)",
    unless = "#result.isEmpty()"
  )
  public List<ProductDto> searchProducts(ProductFilter filter) {
    log.info("Cache MISS - Searching products with filter: {}", filter);
    return productRepository.findByFilter(filter)
      .stream()
      .map(this::toDto)
      .toList();
  }

  // Evict cache khi update
  @CacheEvict(value = "products", key = "#productId")
  public ProductDto updateProduct(Long productId, UpdateProductRequest request) {
    Product product = productRepository.findById(productId)
      .orElseThrow(() -> new NotFoundException("Product not found"));

    product.setName(request.name());
    product.setPrice(request.price());
    Product saved = productRepository.save(product);

    log.info("Cache EVICTED for product: {}", productId);
    return toDto(saved);
  }

  // Evict to√†n b·ªô cache c·ªßa category khi th√™m product m·ªõi
  @CacheEvict(value = "products", allEntries = true)
  public ProductDto createProduct(CreateProductRequest request) {
    Product product = Product.builder()
      .name(request.name())
      .price(request.price())
      .categoryId(request.categoryId())
      .build();

    Product saved = productRepository.save(product);
    log.info("All product caches EVICTED due to new product creation");
    return toDto(saved);
  }

  // CachePut: lu√¥n execute method V√Ä update cache
  @CachePut(value = "products", key = "#result.id")
  public ProductDto refreshProduct(Long productId) {
    log.info("Refreshing cache for product: {}", productId);
    return productRepository.findById(productId)
      .map(this::toDto)
      .orElse(null);
  }

  private ProductDto toDto(Product product) {
    return ProductDto.builder()
      .id(product.getId())
      .name(product.getName())
      .price(product.getPrice())
      .build();
  }
}

// ===== DTO: ProductDto.java =====
@Builder
public record ProductDto(
  Long id,
  String name,
  BigDecimal price
) implements Serializable {
  // Implement Serializable ƒë·ªÉ serialize v√†o Redis
}
```

### ‚ùå C√°ch sai

```java
// ‚ùå SAI 1: Kh√¥ng c√≥ TTL ‚Üí memory leak
@Cacheable(value = "products", key = "#id")
public ProductDto getProduct(Long id) {
  // Cache s·∫Ω t·ªìn t·∫°i m√£i m√£i, g√¢y OutOfMemoryError
}

// ‚ùå SAI 2: Cache key kh√¥ng unique ‚Üí collision
@Cacheable(value = "products", key = "#page")
public Page<ProductDto> getProducts(int page, int size, String category) {
  // Key ch·ªâ c√≥ page ‚Üí c√°c category kh√°c nhau c√πng page s·∫Ω tr·∫£ v·ªÅ data sai
}

// ‚ùå SAI 3: Qu√™n evict cache khi update
public ProductDto updateProduct(Long id, UpdateProductRequest request) {
  Product product = productRepository.findById(id).orElseThrow();
  product.setName(request.name());
  productRepository.save(product);
  // Cache c≈© v·∫´n c√≤n ‚Üí user th·∫•y data c≈©
  return toDto(product);
}

// ‚ùå SAI 4: Cache data nh·∫°y c·∫£m (passwords, tokens)
@Cacheable(value = "users", key = "#userId")
public UserDto getUser(Long userId) {
  // N·∫øu cache b·ªã leak ‚Üí l·ªô password hash, token
}

// ‚ùå SAI 5: Cache exception/null
@Cacheable(value = "products", key = "#id")
public ProductDto getProduct(Long id) {
  // N·∫øu throw exception ‚Üí cache null ‚Üí m·ªçi request sau tr·∫£ null
  return productRepository.findById(id)
    .orElseThrow(() -> new NotFoundException("Product not found"));
}
// FIX: Th√™m unless = "#result == null"

// ‚ùå SAI 6: Kh√¥ng implement Serializable cho DTO
public class ProductDto { // Thi·∫øu implements Serializable
  private Long id;
  private String name;
  // Redis serialize s·∫Ω fail
}

// ‚ùå SAI 7: Self-invocation kh√¥ng trigger cache
@Service
public class ProductService {

  @Cacheable("products")
  public ProductDto getProduct(Long id) {
    return productRepository.findById(id).map(this::toDto).orElse(null);
  }

  public ProductDto getProductInternal(Long id) {
    // ‚ùå G·ªçi method trong c√πng class ‚Üí Spring AOP kh√¥ng intercept
    return this.getProduct(id); // Cache KH√îNG ho·∫°t ƒë·ªông
  }
}
// FIX: Inject ProductService v√†o ch√≠nh n√≥ (self-injection) ho·∫∑c d√πng @Lazy
```

### Ph√°t hi·ªán

**Regex patterns:**
```regex
# Thi·∫øu TTL config
@Cacheable.*\n(?!.*entryTtl)

# Cache key ƒë∆°n gi·∫£n (ch·ªâ 1 param)
@Cacheable.*key\s*=\s*"#\w+"[^+:]*\)

# Thi·∫øu unless/condition
@Cacheable(?!.*unless)(?!.*condition).*\)

# Method c√≥ @Cacheable nh∆∞ng kh√¥ng c√≥ @CacheEvict t∆∞∆°ng ·ª©ng
public.*update.*\{(?!.*@CacheEvict)

# DTO kh√¥ng implement Serializable
(class|record)\s+\w+Dto(?!.*implements\s+Serializable)
```

**Checklist:**
```java
// 1. Config c√≥ TTL cho t·∫•t c·∫£ cache?
@Bean
public CacheManager cacheManager() {
  return RedisCacheManager.builder()
    .cacheDefaults(config.entryTtl(Duration.ofMinutes(10))) // ‚úÖ
    .build();
}

// 2. Cache key ƒë·ªß unique?
@Cacheable(key = "#id + ':' + #locale + ':' + #version") // ‚úÖ

// 3. C√≥ unless ƒë·ªÉ tr√°nh cache null/exception?
@Cacheable(unless = "#result == null || #result.isEmpty()") // ‚úÖ

// 4. Write operations c√≥ evict cache?
@CacheEvict(value = "products", key = "#id") // ‚úÖ
public void updateProduct(Long id) { }

// 5. DTO c√≥ Serializable?
public record ProductDto(...) implements Serializable { } // ‚úÖ

// 6. Tr√°nh cache data nh·∫°y c·∫£m?
@Cacheable("users")
public UserDto getUser(Long id) {
  return UserDto.builder()
    .id(user.getId())
    .email(user.getEmail())
    // ‚úÖ KH√îNG tr·∫£ password, token, creditCard
    .build();
}

// 7. Test cache behavior?
@Test
void testCacheHit() {
  productService.getProduct(1L); // Cache MISS
  productService.getProduct(1L); // Cache HIT
  verify(productRepository, times(1)).findById(1L); // ‚úÖ Ch·ªâ query 1 l·∫ßn
}
```

---

## 10.02 - Cache key strategy r√µ r√†ng (tr√°nh collision) üü†

### Metadata
- **ID:** `CACHE-002`
- **M·ª©c ƒë·ªô:** üü† KHUY·∫æN NGH·ªä
- **L√Ω do:** Cache collision g√¢y tr·∫£ v·ªÅ data sai ‚Üí critical bug
- **Trade-off:** Key ph·ª©c t·∫°p ‚Üí d√†i h∆°n ‚Üí memory overhead (nh·ªè)

### T·∫°i sao?

**V·∫•n ƒë·ªÅ:**
- Key ƒë∆°n gi·∫£n (`#id`) ‚Üí collision khi query kh√°c nhau c√πng param
- Example: `getProduct(1, locale=EN)` v√† `getProduct(1, locale=JP)` c√πng key `1`
- Multi-tenant: tenant A th·∫•y data c·ªßa tenant B
- Pagination: page 1 size 10 vs page 1 size 20 ‚Üí c√πng key `1`

**Gi·∫£i ph√°p:**
- Composite key: `tenantId:entityId:locale:version`
- Prefix theo domain: `product:1`, `user:1` (tr√°nh ID tr√πng)
- Hash key n·∫øu qu√° d√†i (>100 chars)

### ‚úÖ C√°ch ƒë√∫ng

```java
// ===== KeyGenerator: CustomKeyGenerator.java =====
@Component("customKeyGenerator")
public class CustomKeyGenerator implements KeyGenerator {

  @Override
  public Object generate(Object target, Method method, Object... params) {
    // T·∫°o key format: ClassName.methodName(param1,param2,...)
    String className = target.getClass().getSimpleName();
    String methodName = method.getName();
    String paramsKey = Arrays.stream(params)
      .map(param -> param == null ? "null" : param.toString())
      .collect(Collectors.joining(","));

    String rawKey = String.format("%s.%s(%s)", className, methodName, paramsKey);

    // Hash n·∫øu key qu√° d√†i
    if (rawKey.length() > 100) {
      return DigestUtils.md5DigestAsHex(rawKey.getBytes(StandardCharsets.UTF_8));
    }

    return rawKey;
  }
}

// ===== Service Examples =====
@Service
public class ProductService {

  // ‚úÖ Composite key v·ªõi nhi·ªÅu params
  @Cacheable(
    value = "products",
    key = "#tenantId + ':' + #productId + ':' + #locale"
  )
  public ProductDto getProduct(Long tenantId, Long productId, String locale) {
    // Key example: "100:50:en_US"
  }

  // ‚úÖ Prefix theo domain
  @Cacheable(
    value = "entities",
    key = "'product:' + #id" // product:1 vs user:1 ‚Üí kh√°c nhau
  )
  public ProductDto getProductById(Long id) { }

  // ‚úÖ Include all pagination params
  @Cacheable(
    value = "products",
    key = "#categoryId + ':page:' + #page + ':size:' + #size + ':sort:' + #sort"
  )
  public Page<ProductDto> getProducts(
    Long categoryId,
    int page,
    int size,
    String sort
  ) {
    // Key: "10:page:0:size:20:sort:name"
  }

  // ‚úÖ Object param ‚Üí use custom KeyGenerator
  @Cacheable(
    value = "products",
    keyGenerator = "customKeyGenerator"
  )
  public List<ProductDto> searchProducts(ProductSearchFilter filter) {
    // Key: ProductService.searchProducts(ProductSearchFilter{brand=Nike,minPrice=100,...})
  }

  // ‚úÖ SpEL v·ªõi nested object
  @Cacheable(
    value = "products",
    key = "#filter.tenantId + ':' + #filter.brand + ':' + #filter.priceRange.min + '-' + #filter.priceRange.max"
  )
  public List<ProductDto> search(ProductFilter filter) {
    // Key: "100:Nike:50-200"
  }

  // ‚úÖ Hash long key
  @Cacheable(
    value = "reports",
    key = "T(org.springframework.util.DigestUtils).md5DigestAsHex((#params.toString()).getBytes())"
  )
  public ReportDto generateReport(ReportParams params) {
    // Key: md5 hash c·ªßa params.toString()
  }

  // ‚úÖ Version-aware cache key
  @Cacheable(
    value = "products",
    key = "'v' + @appConfig.cacheVersion + ':' + #id"
  )
  public ProductDto getProduct(Long id) {
    // Key: "v2:100" ‚Üí bump version ƒë·ªÉ invalidate all cache
  }

  // ‚úÖ Multi-tenant v·ªõi security context
  @Cacheable(
    value = "users",
    key = "T(org.springframework.security.core.context.SecurityContextHolder).getContext().getAuthentication().getName() + ':' + #userId"
  )
  public UserDto getUser(Long userId) {
    // Key: "admin@example.com:100"
  }
}

// ===== Config: AppConfig.java =====
@Configuration
@ConfigurationProperties(prefix = "app")
@Data
public class AppConfig {
  private int cacheVersion = 1; // Bump ƒë·ªÉ invalidate all cache
}
```

### ‚ùå C√°ch sai

```java
// ‚ùå SAI 1: Key ch·ªâ c√≥ 1 param (thi·∫øu locale, tenant, version)
@Cacheable(value = "products", key = "#productId")
public ProductDto getProduct(Long tenantId, Long productId, String locale) {
  // Tenant A locale EN vs Tenant B locale JP ‚Üí c√πng key ‚Üí SAI
}

// ‚ùå SAI 2: Pagination key thi·∫øu size/sort
@Cacheable(value = "products", key = "#page")
public Page<ProductDto> getProducts(int page, int size, String sort) {
  // page=1, size=10 vs page=1, size=20 ‚Üí c√πng key "1"
}

// ‚ùå SAI 3: Object param kh√¥ng override toString()
@Cacheable(value = "products", key = "#filter")
public List<ProductDto> search(ProductFilter filter) {
  // Key = filter.toString() = "ProductFilter@a3f5b" (hashCode) ‚Üí kh√¥ng stable
}

// ‚ùå SAI 4: Kh√¥ng prefix theo domain
@Cacheable(value = "cache", key = "#id")
public ProductDto getProduct(Long id) { }

@Cacheable(value = "cache", key = "#id")
public UserDto getUser(Long id) { }
// getProduct(1) v√† getUser(1) ‚Üí c√πng key "1" ‚Üí collision

// ‚ùå SAI 5: Key d√†i kh√¥ng hash
@Cacheable(
  value = "reports",
  key = "#p1 + #p2 + #p3 + ... + #p50" // Key > 500 chars ‚Üí memory waste
)
public ReportDto generate(...50 params) { }

// ‚ùå SAI 6: Null-unsafe key
@Cacheable(value = "products", key = "#categoryId + ':' + #brandId")
public List<ProductDto> getProducts(Long categoryId, Long brandId) {
  // brandId = null ‚Üí key = "10:null" (String) vs null (object) ‚Üí inconsistent
}
// FIX: key = "#categoryId + ':' + (#brandId != null ? #brandId : 'all')"

// ‚ùå SAI 7: Multi-tenant kh√¥ng include tenantId
@Cacheable(value = "users", key = "#userId")
public UserDto getUser(Long userId) {
  Long tenantId = TenantContext.getCurrentTenantId();
  // Tenant A th·∫•y data c·ªßa Tenant B ‚Üí CRITICAL BUG
}
```

### Ph√°t hi·ªán

**Regex patterns:**
```regex
# Key ch·ªâ c√≥ 1 param ƒë∆°n gi·∫£n
@Cacheable.*key\s*=\s*"#\w+"[^\+:]*\)

# Pagination key thi·∫øu size/sort
@Cacheable.*key.*page(?!.*size)

# Object param kh√¥ng d√πng keyGenerator
@Cacheable.*key\s*=\s*"#\w+Filter"(?!.*keyGenerator)

# Multi-tenant method thi·∫øu tenantId trong key
@Cacheable(?=.*key)(?!.*tenantId).*getTenant|getUser|getProduct

# Key d√†i kh√¥ng hash
@Cacheable.*key.*\+.*\+.*\+.*\+.*\+.*\+ # 6+ concatenations
```

**Checklist:**
```java
// 1. Key c√≥ ƒë·ªß t·∫•t c·∫£ discriminator params?
@Cacheable(key = "#tenantId + ':' + #id + ':' + #locale + ':' + #version") // ‚úÖ

// 2. Pagination key ƒë·∫ßy ƒë·ªß?
@Cacheable(key = "#page + ':' + #size + ':' + #sort") // ‚úÖ

// 3. Object param d√πng keyGenerator?
@Cacheable(keyGenerator = "customKeyGenerator") // ‚úÖ

// 4. Prefix theo domain?
@Cacheable(key = "'product:' + #id") // ‚úÖ

// 5. Hash n·∫øu key > 100 chars?
if (key.length() > 100) return md5(key); // ‚úÖ

// 6. Null-safe?
key = "#id + ':' + (#category != null ? #category : 'all')" // ‚úÖ

// 7. Test cache isolation?
@Test
void testCacheIsolation() {
  productService.getProduct(tenant1, 1L, "en"); // Cache MISS
  productService.getProduct(tenant2, 1L, "en"); // Cache MISS (kh√°c tenant)
  verify(repo, times(2)).findById(1L); // ‚úÖ 2 queries
}
```

---

## 10.03 - TTL (Time-To-Live) cho m·ªçi cache entry üî¥

### Metadata
- **ID:** `CACHE-003`
- **M·ª©c ƒë·ªô:** üî¥ B·∫ÆT BU·ªòC
- **L√Ω do:** Kh√¥ng c√≥ TTL ‚Üí memory leak ‚Üí OutOfMemoryError
- **Trade-off:** TTL ng·∫Øn ‚Üí cache miss nhi·ªÅu, TTL d√†i ‚Üí stale data

### T·∫°i sao?

**V·∫•n ƒë·ªÅ:**
- Cache kh√¥ng expire ‚Üí memory tƒÉng kh√¥ng gi·ªõi h·∫°n
- Stale data t·ªìn t·∫°i m√£i ‚Üí business logic sai
- Redis maxmemory-policy noeviction ‚Üí write fail khi ƒë·∫ßy

**Gi·∫£i ph√°p:**
- Default TTL cho t·∫•t c·∫£ cache (10-60 ph√∫t)
- Custom TTL theo data type:
  - Static data (categories): 1 ng√†y - 1 tu·∫ßn
  - Dynamic data (user profile): 5-15 ph√∫t
  - Real-time data: KH√îNG cache ho·∫∑c < 1 ph√∫t

### ‚úÖ C√°ch ƒë√∫ng

```java
// ===== Config: CacheConfig.java =====
@Configuration
@EnableCaching
public class CacheConfig {

  @Bean
  public RedisCacheManager cacheManager(RedisConnectionFactory factory) {
    // Default config cho t·∫•t c·∫£ cache
    RedisCacheConfiguration defaultConfig = RedisCacheConfiguration
      .defaultCacheConfig()
      .entryTtl(Duration.ofMinutes(10)) // ‚úÖ Default TTL 10 ph√∫t
      .serializeKeysWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new StringRedisSerializer()))
      .serializeValuesWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new GenericJackson2JsonRedisSerializer()))
      .disableCachingNullValues(); // Kh√¥ng cache null

    // Custom TTL cho t·ª´ng cache
    Map<String, RedisCacheConfiguration> cacheConfigs = Map.of(
      // Static data - TTL d√†i
      "categories", defaultConfig.entryTtl(Duration.ofDays(7)),
      "countries", defaultConfig.entryTtl(Duration.ofDays(30)),
      "appSettings", defaultConfig.entryTtl(Duration.ofHours(24)),

      // Semi-static data
      "products", defaultConfig.entryTtl(Duration.ofHours(1)),
      "productCatalog", defaultConfig.entryTtl(Duration.ofMinutes(30)),

      // Dynamic data - TTL ng·∫Øn
      "userProfiles", defaultConfig.entryTtl(Duration.ofMinutes(15)),
      "userSessions", defaultConfig.entryTtl(Duration.ofMinutes(30)),
      "shoppingCarts", defaultConfig.entryTtl(Duration.ofHours(2)),

      // Real-time data - TTL r·∫•t ng·∫Øn
      "stockPrices", defaultConfig.entryTtl(Duration.ofSeconds(30)),
      "onlineUsers", defaultConfig.entryTtl(Duration.ofMinutes(1))
    );

    return RedisCacheManager.builder(factory)
      .cacheDefaults(defaultConfig)
      .withInitialCacheConfigurations(cacheConfigs)
      .transactionAware() // Cache operations trong transaction
      .build();
  }

  // ===== Config cho Redis maxmemory policy =====
  @Bean
  public RedisTemplate<String, Object> redisTemplate(
    RedisConnectionFactory factory
  ) {
    RedisTemplate<String, Object> template = new RedisTemplate<>();
    template.setConnectionFactory(factory);

    // Serialize keys as strings
    template.setKeySerializer(new StringRedisSerializer());
    template.setHashKeySerializer(new StringRedisSerializer());

    // Serialize values as JSON
    GenericJackson2JsonRedisSerializer serializer =
      new GenericJackson2JsonRedisSerializer();
    template.setValueSerializer(serializer);
    template.setHashValueSerializer(serializer);

    return template;
  }
}

// ===== application.yml =====
/*
spring:
  redis:
    host: localhost
    port: 6379
    timeout: 2000ms
    lettuce:
      pool:
        max-active: 8
        max-idle: 8
        min-idle: 2
        max-wait: 2000ms
  cache:
    type: redis
    redis:
      time-to-live: 600000 # Default 10 ph√∫t (milliseconds)
      cache-null-values: false
      use-key-prefix: true
      key-prefix: "myapp:"

# Redis maxmemory policy (config trong redis.conf)
# maxmemory 1gb
# maxmemory-policy allkeys-lru # LRU eviction khi ƒë·∫ßy
*/

// ===== Service: Dynamic TTL =====
@Service
public class CacheService {

  @Autowired
  private CacheManager cacheManager;

  // Dynamic TTL d·ª±a tr√™n business logic
  public void cacheWithDynamicTtl(String cacheName, String key, Object value, Duration ttl) {
    Cache cache = cacheManager.getCache(cacheName);
    if (cache != null) {
      // V·ªõi Spring Boot 3.x + Redis, c·∫ßn d√πng RedisTemplate ƒë·ªÉ set custom TTL
      RedisCache redisCache = (RedisCache) cache;
      redisCache.put(key, value);

      // Set TTL manually (n·∫øu c·∫ßn override default)
      // C·∫ßn inject RedisTemplate
    }
  }

  // Cache v·ªõi conditional TTL
  public void cacheUser(Long userId, UserDto user) {
    Duration ttl = user.isPremium()
      ? Duration.ofHours(1)  // Premium user cache l√¢u h∆°n
      : Duration.ofMinutes(15); // Free user cache ng·∫Øn

    cacheWithDynamicTtl("users", userId.toString(), user, ttl);
  }
}

// ===== Service: TTL-aware caching =====
@Service
public class ProductService {

  @Autowired
  private ProductRepository productRepository;

  // Cache v·ªõi TTL trong annotation (config-driven)
  @Cacheable(value = "products", key = "#id")
  public ProductDto getProduct(Long id) {
    // TTL = 1 hour (theo config)
    return productRepository.findById(id)
      .map(this::toDto)
      .orElse(null);
  }

  // Manual cache v·ªõi custom TTL
  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  public void cacheProductWithCustomTtl(Long id, ProductDto product, long ttlMinutes) {
    String key = "product:" + id;
    redisTemplate.opsForValue().set(key, product, Duration.ofMinutes(ttlMinutes));
  }

  // Get v·ªõi fallback n·∫øu expired
  public ProductDto getProductWithFallback(Long id) {
    String key = "product:" + id;
    ProductDto cached = (ProductDto) redisTemplate.opsForValue().get(key);

    if (cached != null) {
      return cached;
    }

    // Cache miss ‚Üí query DB
    ProductDto fresh = productRepository.findById(id)
      .map(this::toDto)
      .orElse(null);

    if (fresh != null) {
      redisTemplate.opsForValue().set(key, fresh, Duration.ofHours(1));
    }

    return fresh;
  }
}

// ===== Monitoring: Cache TTL metrics =====
@Component
public class CacheMetrics {

  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  @Scheduled(fixedRate = 60000) // Check m·ªói ph√∫t
  public void monitorCacheTtl() {
    Set<String> keys = redisTemplate.keys("myapp:*");

    if (keys != null) {
      keys.forEach(key -> {
        Long ttl = redisTemplate.getExpire(key, TimeUnit.SECONDS);

        if (ttl != null && ttl == -1) {
          // TTL = -1 ‚Üí key kh√¥ng expire ‚Üí WARNING
          log.warn("Cache key without TTL detected: {}", key);
        }
      });
    }
  }
}
```

### ‚ùå C√°ch sai

```java
// ‚ùå SAI 1: Kh√¥ng config TTL
@Configuration
@EnableCaching
public class CacheConfig {
  @Bean
  public CacheManager cacheManager(RedisConnectionFactory factory) {
    return RedisCacheManager.builder(factory)
      .cacheDefaults(RedisCacheConfiguration.defaultCacheConfig())
      // ‚ùå Thi·∫øu .entryTtl() ‚Üí cache t·ªìn t·∫°i m√£i
      .build();
  }
}

// ‚ùå SAI 2: TTL = -1 (never expire)
RedisCacheConfiguration config = RedisCacheConfiguration
  .defaultCacheConfig()
  .entryTtl(Duration.ofSeconds(-1)); // ‚ùå NEVER EXPIRE

// ‚ùå SAI 3: TTL qu√° d√†i cho dynamic data
Map<String, RedisCacheConfiguration> configs = Map.of(
  "userSessions", config.entryTtl(Duration.ofDays(365)) // ‚ùå 1 nƒÉm?!
);

// ‚ùå SAI 4: TTL qu√° ng·∫Øn cho static data
Map<String, RedisCacheConfiguration> configs = Map.of(
  "countries", config.entryTtl(Duration.ofSeconds(10)) // ‚ùå 10s ‚Üí cache thrashing
);

// ‚ùå SAI 5: Kh√¥ng monitor TTL = -1 keys
// Redis command: KEYS * ‚Üí n·∫øu th·∫•y key kh√¥ng expire ‚Üí memory leak

// ‚ùå SAI 6: Redis maxmemory-policy = noeviction
/*
# redis.conf
maxmemory 1gb
maxmemory-policy noeviction  # ‚ùå Khi ƒë·∫ßy ‚Üí write fail ‚Üí app crash
*/
// FIX: d√πng allkeys-lru ho·∫∑c volatile-lru

// ‚ùå SAI 7: Cache data l·ªõn v·ªõi TTL d√†i
@Cacheable(value = "reports", key = "#reportId")
public byte[] generateLargeReport(Long reportId) {
  // Report 50MB, TTL 1 ng√†y ‚Üí 1000 reports = 50GB RAM
  return generateReport(reportId);
}
// FIX: Cache link to S3, kh√¥ng cache binary data
```

### Ph√°t hi·ªán

**Regex patterns:**
```regex
# Config kh√¥ng c√≥ entryTtl
@Bean.*CacheManager(?!.*entryTtl).*\{

# TTL = -1 ho·∫∑c qu√° d√†i
\.entryTtl\(Duration\.of(Days|Hours)\((365|999|[5-9]\d{2})\)

# Cache l·ªõn (byte[], InputStream) kh√¥ng gi·ªõi h·∫°n TTL
@Cacheable.*\n.*public\s+(byte\[\]|InputStream)

# Redis config thi·∫øu maxmemory-policy
# C·∫ßn check redis.conf manually
```

**Checklist:**
```java
// 1. Default TTL ƒë∆∞·ª£c config?
.cacheDefaults(config.entryTtl(Duration.ofMinutes(10))) // ‚úÖ

// 2. Custom TTL cho t·ª´ng cache type?
Map<String, RedisCacheConfiguration> configs = Map.of(
  "static", config.entryTtl(Duration.ofDays(7)),
  "dynamic", config.entryTtl(Duration.ofMinutes(15))
); // ‚úÖ

// 3. TTL h·ª£p l√Ω?
// Static (categories, settings): 1 gi·ªù - 7 ng√†y ‚úÖ
// Dynamic (users, sessions): 5-30 ph√∫t ‚úÖ
// Real-time (prices, status): 10s - 1 ph√∫t ‚úÖ

// 4. Monitor TTL = -1 keys?
@Scheduled(fixedRate = 60000)
public void checkNoTtlKeys() {
  // Scan keys with TTL = -1
} // ‚úÖ

// 5. Redis maxmemory-policy config?
# redis.conf
maxmemory 2gb
maxmemory-policy allkeys-lru # ‚úÖ

// 6. Test cache expiration?
@Test
void testCacheExpiration() throws InterruptedException {
  service.getData(1L); // Cache
  Thread.sleep(Duration.ofMinutes(11).toMillis()); // Wait TTL
  service.getData(1L); // Should query DB again
  verify(repo, times(2)).findById(1L); // ‚úÖ
}
```

---

## 10.04 - Cache invalidation khi data thay ƒë·ªïi üî¥

### Metadata
- **ID:** `CACHE-004`
- **M·ª©c ƒë·ªô:** üî¥ B·∫ÆT BU·ªòC
- **L√Ω do:** Stale cache ‚Üí user th·∫•y data c≈© ‚Üí critical bug, business loss
- **Trade-off:** Invalidation ph·ª©c t·∫°p ‚Üí c√≥ th·ªÉ invalidate qu√° nhi·ªÅu ‚Üí cache miss

### T·∫°i sao?

**V·∫•n ƒë·ªÅ:**
- Update product price ‚Üí cache v·∫´n gi·ªØ gi√° c≈© ‚Üí user mua sai gi√°
- Delete user ‚Üí cache v·∫´n tr·∫£ user ‚Üí security issue
- Cascade invalidation: update category ‚Üí invalidate all products trong category

**Gi·∫£i ph√°p:**
- `@CacheEvict`: x√≥a cache khi update/delete
- `allEntries = true`: x√≥a to√†n b·ªô cache (d√πng khi update ·∫£nh h∆∞·ªüng nhi·ªÅu entries)
- `beforeInvocation = true`: evict TR∆Ø·ªöC khi execute method (n·∫øu method throw exception)
- Event-driven invalidation: publish event ‚Üí listener invalidate cache

### ‚úÖ C√°ch ƒë√∫ng

```java
// ===== Service: ProductService.java =====
@Service
@Slf4j
public class ProductService {

  @Autowired
  private ProductRepository productRepository;

  @Autowired
  private ApplicationEventPublisher eventPublisher;

  // ===== READ: Cacheable =====
  @Cacheable(value = "products", key = "#id", unless = "#result == null")
  public ProductDto getProduct(Long id) {
    return productRepository.findById(id)
      .map(this::toDto)
      .orElse(null);
  }

  @Cacheable(
    value = "products:category",
    key = "#categoryId + ':' + #page + ':' + #size"
  )
  public Page<ProductDto> getProductsByCategory(Long categoryId, int page, int size) {
    Pageable pageable = PageRequest.of(page, size);
    return productRepository.findByCategoryId(categoryId, pageable)
      .map(this::toDto);
  }

  // ===== UPDATE: Evict single entry =====
  @CacheEvict(value = "products", key = "#id")
  public ProductDto updateProduct(Long id, UpdateProductRequest request) {
    Product product = productRepository.findById(id)
      .orElseThrow(() -> new NotFoundException("Product not found: " + id));

    Long oldCategoryId = product.getCategoryId();

    product.setName(request.name());
    product.setPrice(request.price());
    product.setCategoryId(request.categoryId());

    Product saved = productRepository.save(product);

    // Invalidate category cache n·∫øu category thay ƒë·ªïi
    if (!Objects.equals(oldCategoryId, request.categoryId())) {
      eventPublisher.publishEvent(new CategoryChangedEvent(
        oldCategoryId,
        request.categoryId()
      ));
    }

    log.info("Cache evicted for product: {}", id);
    return toDto(saved);
  }

  // ===== DELETE: Evict + cascade =====
  @CacheEvict(value = "products", key = "#id")
  public void deleteProduct(Long id) {
    Product product = productRepository.findById(id)
      .orElseThrow(() -> new NotFoundException("Product not found: " + id));

    productRepository.delete(product);

    // Cascade invalidation
    eventPublisher.publishEvent(new ProductDeletedEvent(id, product.getCategoryId()));

    log.info("Product deleted and cache evicted: {}", id);
  }

  // ===== CREATE: Evict all (v√¨ list APIs s·∫Ω thay ƒë·ªïi) =====
  @CacheEvict(value = "products:category", allEntries = true)
  public ProductDto createProduct(CreateProductRequest request) {
    Product product = Product.builder()
      .name(request.name())
      .price(request.price())
      .categoryId(request.categoryId())
      .build();

    Product saved = productRepository.save(product);

    log.info("Product created, all category caches evicted");
    return toDto(saved);
  }

  // ===== BULK UPDATE: Multiple evictions =====
  @Caching(evict = {
    @CacheEvict(value = "products", allEntries = true),
    @CacheEvict(value = "products:category", allEntries = true)
  })
  public void bulkUpdatePrices(List<Long> productIds, BigDecimal discountPercent) {
    productRepository.findAllById(productIds).forEach(product -> {
      BigDecimal newPrice = product.getPrice()
        .multiply(BigDecimal.ONE.subtract(discountPercent));
      product.setPrice(newPrice);
    });

    productRepository.flush();
    log.info("Bulk update completed, all caches evicted");
  }

  // ===== Transaction-aware eviction =====
  @Transactional
  @CacheEvict(value = "products", key = "#id")
  public ProductDto updateProductTransactional(Long id, UpdateProductRequest request) {
    // Eviction ch·ªâ trigger SAU KHI transaction commit th√†nh c√¥ng
    Product product = productRepository.findById(id).orElseThrow();
    product.setName(request.name());
    return toDto(productRepository.save(product));

    // N·∫øu throw exception ‚Üí rollback ‚Üí cache KH√îNG b·ªã evict
  }

  // ===== beforeInvocation: Evict tr∆∞·ªõc khi execute =====
  @CacheEvict(
    value = "products",
    key = "#id",
    beforeInvocation = true // Evict TR∆Ø·ªöC khi method ch·∫°y
  )
  public void updateProductUnsafe(Long id, UpdateProductRequest request) {
    // N·∫øu method n√†y throw exception ‚Üí cache ƒë√£ b·ªã evict
    // D√πng khi kh√¥ng mu·ªën cache inconsistent state
    Product product = productRepository.findById(id).orElseThrow();
    product.setName(request.name());
    productRepository.save(product);
  }
}

// ===== Event: CategoryChangedEvent.java =====
public record CategoryChangedEvent(
  Long oldCategoryId,
  Long newCategoryId
) { }

// ===== Listener: CacheInvalidationListener.java =====
@Component
@Slf4j
public class CacheInvalidationListener {

  @Autowired
  private CacheManager cacheManager;

  @EventListener
  public void onCategoryChanged(CategoryChangedEvent event) {
    // Invalidate cache c·ªßa c·∫£ 2 categories
    evictCategoryCache(event.oldCategoryId());
    evictCategoryCache(event.newCategoryId());
  }

  @EventListener
  public void onProductDeleted(ProductDeletedEvent event) {
    // Invalidate category cache
    evictCategoryCache(event.categoryId());
  }

  private void evictCategoryCache(Long categoryId) {
    Cache cache = cacheManager.getCache("products:category");
    if (cache != null) {
      // Evict all entries c√≥ categoryId
      cache.clear(); // Ho·∫∑c d√πng pattern matching n·∫øu Redis
      log.info("Category cache evicted for categoryId: {}", categoryId);
    }
  }

  // ===== Pattern-based eviction v·ªõi Redis =====
  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  public void evictByPattern(String pattern) {
    Set<String> keys = redisTemplate.keys(pattern);
    if (keys != null && !keys.isEmpty()) {
      redisTemplate.delete(keys);
      log.info("Evicted {} keys matching pattern: {}", keys.size(), pattern);
    }
  }

  // Example: evict all products in category
  public void evictProductsByCategory(Long categoryId) {
    evictByPattern("products:category:" + categoryId + ":*");
  }
}

// ===== Manual cache eviction service =====
@Service
public class CacheEvictionService {

  @Autowired
  private CacheManager cacheManager;

  public void evictAllCaches() {
    cacheManager.getCacheNames().forEach(cacheName -> {
      Cache cache = cacheManager.getCache(cacheName);
      if (cache != null) {
        cache.clear();
        log.info("Cache cleared: {}", cacheName);
      }
    });
  }

  public void evictCacheByName(String cacheName) {
    Cache cache = cacheManager.getCache(cacheName);
    if (cache != null) {
      cache.clear();
      log.info("Cache cleared: {}", cacheName);
    }
  }

  public void evictCacheEntry(String cacheName, Object key) {
    Cache cache = cacheManager.getCache(cacheName);
    if (cache != null) {
      cache.evict(key);
      log.info("Cache entry evicted: {}:{}", cacheName, key);
    }
  }
}

// ===== Admin endpoint ƒë·ªÉ manual evict =====
@RestController
@RequestMapping("/api/admin/cache")
public class CacheAdminController {

  @Autowired
  private CacheEvictionService cacheEvictionService;

  @PostMapping("/evict-all")
  @PreAuthorize("hasRole('ADMIN')")
  public ResponseEntity<String> evictAll() {
    cacheEvictionService.evictAllCaches();
    return ResponseEntity.ok("All caches evicted");
  }

  @PostMapping("/evict/{cacheName}")
  @PreAuthorize("hasRole('ADMIN')")
  public ResponseEntity<String> evictCache(@PathVariable String cacheName) {
    cacheEvictionService.evictCacheByName(cacheName);
    return ResponseEntity.ok("Cache evicted: " + cacheName);
  }
}
```

### ‚ùå C√°ch sai

```java
// ‚ùå SAI 1: Update/delete kh√¥ng evict cache
public ProductDto updateProduct(Long id, UpdateProductRequest request) {
  Product product = productRepository.findById(id).orElseThrow();
  product.setName(request.name());
  productRepository.save(product);
  // ‚ùå Thi·∫øu @CacheEvict ‚Üí cache v·∫´n gi·ªØ data c≈©
  return toDto(product);
}

// ‚ùå SAI 2: Evict sai key
@CacheEvict(value = "products", key = "#request.id") // ‚ùå Sai param
public ProductDto updateProduct(Long id, UpdateProductRequest request) {
  // Key cache l√† #id, nh∆∞ng evict d√πng #request.id
}

// ‚ùå SAI 3: Cascade invalidation kh√¥ng ƒë·ªß
@CacheEvict(value = "products", key = "#id")
public void deleteProduct(Long id) {
  productRepository.deleteById(id);
  // ‚ùå Thi·∫øu evict "products:category" cache
  // ‚Üí List products by category v·∫´n hi·ªÉn th·ªã product ƒë√£ x√≥a
}

// ‚ùå SAI 4: Transaction rollback nh∆∞ng cache ƒë√£ evict
@Transactional
@CacheEvict(value = "products", key = "#id", beforeInvocation = true)
public void updateProduct(Long id, UpdateProductRequest request) {
  Product product = productRepository.findById(id).orElseThrow();
  product.setName(request.name());
  productRepository.save(product);

  if (someCondition) {
    throw new RuntimeException("Rollback!"); // Transaction rollback
    // ‚ùå Cache ƒë√£ b·ªã evict (beforeInvocation = true)
    // ‚Üí Cache miss ‚Üí query DB ‚Üí l·∫•y data c≈© ‚Üí cache l·∫°i ‚Üí OK
    // Nh∆∞ng n·∫øu c√≥ concurrent request ‚Üí c√≥ th·ªÉ cache data inconsistent
  }
}

// ‚ùå SAI 5: Evict kh√¥ng ƒë·ªß trong bulk update
@CacheEvict(value = "products", key = "#productIds[0]") // ‚ùå Ch·ªâ evict 1 product
public void bulkUpdatePrices(List<Long> productIds, BigDecimal discount) {
  // Update 100 products nh∆∞ng ch·ªâ evict 1
}

// ‚ùå SAI 6: Kh√¥ng evict related caches
@CacheEvict(value = "products", key = "#id")
public void updateProductCategory(Long id, Long newCategoryId) {
  Product product = productRepository.findById(id).orElseThrow();
  product.setCategoryId(newCategoryId);
  productRepository.save(product);
  // ‚ùå Thi·∫øu evict:
  // - "products:category:{oldCategoryId}" cache
  // - "products:category:{newCategoryId}" cache
  // - "categories" cache (n·∫øu c√≥ product count)
}

// ‚ùå SAI 7: Race condition trong eviction
public void updateProductConcurrent(Long id, String newName) {
  Product product = productRepository.findById(id).orElseThrow();
  product.setName(newName);
  productRepository.save(product);

  // Manual evict SAU KHI save
  Cache cache = cacheManager.getCache("products");
  cache.evict(id);

  // ‚ùå Race condition:
  // T1: save() ‚Üí evict()
  // T2:          getProduct() (cache old data) ‚Üê T1 ch∆∞a evict xong
  // ‚Üí T2 cache data c≈©
}
// FIX: D√πng @CacheEvict annotation (atomic)
```

### Ph√°t hi·ªán

**Regex patterns:**
```regex
# Update/delete method thi·∫øu @CacheEvict
public.*\b(update|delete|remove)\w+\(.*\)(?!.*@CacheEvict)

# @Cacheable c√≥ nh∆∞ng kh√¥ng c√≥ @CacheEvict t∆∞∆°ng ·ª©ng
@Cacheable.*value\s*=\s*"(\w+)"(?!.*@CacheEvict.*value\s*=\s*"\1")

# beforeInvocation = true trong @Transactional
@Transactional.*\n.*@CacheEvict.*beforeInvocation\s*=\s*true

# Bulk operation evict 1 entry
@CacheEvict.*key.*\[0\].*\n.*public.*bulk
```

**Checklist:**
```java
// 1. M·ªçi update/delete c√≥ @CacheEvict?
@CacheEvict(value = "products", key = "#id") // ‚úÖ
public void updateProduct(Long id, ...) { }

// 2. Key eviction kh·ªõp v·ªõi key cache?
@Cacheable(key = "#id") + @CacheEvict(key = "#id") // ‚úÖ

// 3. Cascade invalidation ƒë·∫ßy ƒë·ªß?
@Caching(evict = {
  @CacheEvict(value = "products", key = "#id"),
  @CacheEvict(value = "products:category", allEntries = true)
}) // ‚úÖ

// 4. beforeInvocation ph√π h·ª£p?
@CacheEvict(beforeInvocation = false) // Default, evict SAU transaction commit ‚úÖ

// 5. Bulk operation evict all?
@CacheEvict(allEntries = true) // ‚úÖ
public void bulkUpdate(...) { }

// 6. Event-driven invalidation cho complex scenario?
eventPublisher.publishEvent(new ProductUpdatedEvent(...)); // ‚úÖ

// 7. Admin endpoint ƒë·ªÉ manual evict?
@PostMapping("/admin/cache/evict-all") // ‚úÖ

// 8. Test cache invalidation?
@Test
void testCacheEviction() {
  service.getProduct(1L); // Cache
  service.updateProduct(1L, request); // Evict
  service.getProduct(1L); // Cache MISS
  verify(repo, times(2)).findById(1L); // ‚úÖ 2 queries
}
```

---

## 10.05 - Tr√°nh cache stampede (singleflight / lock) üü†

### Metadata
- **ID:** `CACHE-005`
- **M·ª©c ƒë·ªô:** üü† KHUY·∫æN NGH·ªä
- **L√Ω do:** Cache expire ‚Üí 1000 concurrent requests ‚Üí 1000 DB queries ‚Üí DB overload
- **Trade-off:** Lock/singleflight ‚Üí request ƒë·∫ßu ti√™n ch·∫≠m h∆°n, complexity tƒÉng

### T·∫°i sao?

**V·∫•n ƒë·ªÅ: Cache stampede (thundering herd)**
- Cache expire t·∫°i th·ªùi ƒëi·ªÉm peak traffic
- 1000 requests c√πng l√∫c th·∫•y cache miss
- T·∫•t c·∫£ query DB ‚Üí DB connection pool exhausted ‚Üí timeout ‚Üí cascade failure

**Gi·∫£i ph√°p:**
1. **Singleflight pattern**: ch·ªâ 1 request query DB, c√°c request kh√°c ƒë·ª£i k·∫øt qu·∫£
2. **Lock-based**: distributed lock (Redis SETNX) ƒë·ªÉ ch·∫∑n concurrent queries
3. **Probabilistic early expiration**: refresh cache tr∆∞·ªõc khi expire (XFetch)
4. **Cache warming**: pre-load cache khi app start

### ‚úÖ C√°ch ƒë√∫ng

```java
// ===== 1. Singleflight Pattern v·ªõi CompletableFuture =====
@Service
@Slf4j
public class SingleflightCacheService {

  private final Map<String, CompletableFuture<Object>> inflightRequests =
    new ConcurrentHashMap<>();

  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  @Autowired
  private ProductRepository productRepository;

  public ProductDto getProduct(Long productId) {
    String cacheKey = "product:" + productId;

    // 1. Check cache
    ProductDto cached = (ProductDto) redisTemplate.opsForValue().get(cacheKey);
    if (cached != null) {
      return cached;
    }

    // 2. Singleflight: ch·ªâ 1 request query DB
    CompletableFuture<Object> future = inflightRequests.computeIfAbsent(
      cacheKey,
      key -> CompletableFuture.supplyAsync(() -> {
        log.info("Cache MISS, querying DB for: {}", productId);

        ProductDto product = productRepository.findById(productId)
          .map(this::toDto)
          .orElse(null);

        if (product != null) {
          redisTemplate.opsForValue().set(cacheKey, product, Duration.ofHours(1));
        }

        return product;
      }).whenComplete((result, ex) -> {
        // Remove t·ª´ inflight sau khi ho√†n th√†nh
        inflightRequests.remove(cacheKey);
      })
    );

    try {
      return (ProductDto) future.get(5, TimeUnit.SECONDS);
    } catch (Exception e) {
      log.error("Error getting product from singleflight", e);
      inflightRequests.remove(cacheKey);
      throw new RuntimeException("Failed to get product", e);
    }
  }

  private ProductDto toDto(Product product) {
    return ProductDto.builder()
      .id(product.getId())
      .name(product.getName())
      .build();
  }
}

// ===== 2. Distributed Lock v·ªõi Redisson =====
@Service
@Slf4j
public class DistributedLockCacheService {

  @Autowired
  private RedissonClient redissonClient;

  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  @Autowired
  private ProductRepository productRepository;

  public ProductDto getProduct(Long productId) {
    String cacheKey = "product:" + productId;

    // 1. Check cache
    ProductDto cached = (ProductDto) redisTemplate.opsForValue().get(cacheKey);
    if (cached != null) {
      return cached;
    }

    // 2. Acquire distributed lock
    String lockKey = "lock:product:" + productId;
    RLock lock = redissonClient.getLock(lockKey);

    try {
      // Wait max 5s ƒë·ªÉ acquire lock, lock t·ª± release sau 10s
      boolean acquired = lock.tryLock(5, 10, TimeUnit.SECONDS);

      if (!acquired) {
        log.warn("Failed to acquire lock for: {}", productId);
        throw new RuntimeException("Too many concurrent requests");
      }

      // Double-check cache (c√≥ th·ªÉ thread kh√°c ƒë√£ load)
      cached = (ProductDto) redisTemplate.opsForValue().get(cacheKey);
      if (cached != null) {
        return cached;
      }

      // Query DB
      log.info("Lock acquired, querying DB for: {}", productId);
      ProductDto product = productRepository.findById(productId)
        .map(this::toDto)
        .orElse(null);

      if (product != null) {
        redisTemplate.opsForValue().set(cacheKey, product, Duration.ofHours(1));
      }

      return product;

    } catch (InterruptedException e) {
      Thread.currentThread().interrupt();
      throw new RuntimeException("Lock interrupted", e);
    } finally {
      if (lock.isHeldByCurrentThread()) {
        lock.unlock();
      }
    }
  }

  private ProductDto toDto(Product product) {
    return ProductDto.builder()
      .id(product.getId())
      .name(product.getName())
      .build();
  }
}

// ===== 3. Probabilistic Early Expiration (XFetch) =====
@Service
@Slf4j
public class XFetchCacheService {

  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  @Autowired
  private ProductRepository productRepository;

  private static final double BETA = 1.0; // Tuning parameter

  public ProductDto getProduct(Long productId) {
    String cacheKey = "product:" + productId;
    long now = System.currentTimeMillis();

    // Get cache v·ªõi timestamp
    CachedValue<ProductDto> cached = (CachedValue<ProductDto>)
      redisTemplate.opsForValue().get(cacheKey);

    if (cached != null) {
      long delta = now - cached.cachedAt();
      long ttl = cached.ttl();

      // XFetch formula: random() < delta * BETA / TTL
      double probability = (double) delta * BETA / ttl;

      if (Math.random() < probability) {
        log.info("Probabilistic early refresh for: {}", productId);
        return refreshCache(productId, cacheKey);
      }

      return cached.value();
    }

    // Cache miss
    return refreshCache(productId, cacheKey);
  }

  private ProductDto refreshCache(Long productId, String cacheKey) {
    ProductDto product = productRepository.findById(productId)
      .map(this::toDto)
      .orElse(null);

    if (product != null) {
      long ttl = Duration.ofHours(1).toMillis();
      CachedValue<ProductDto> cachedValue = new CachedValue<>(
        product,
        System.currentTimeMillis(),
        ttl
      );
      redisTemplate.opsForValue().set(cacheKey, cachedValue, Duration.ofMillis(ttl));
    }

    return product;
  }

  private ProductDto toDto(Product product) {
    return ProductDto.builder()
      .id(product.getId())
      .name(product.getName())
      .build();
  }

  @Builder
  private record CachedValue<T>(
    T value,
    long cachedAt,
    long ttl
  ) implements Serializable { }
}

// ===== 4. Cache Warming on Startup =====
@Component
@Slf4j
public class CacheWarmer {

  @Autowired
  private ProductRepository productRepository;

  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  @EventListener(ApplicationReadyEvent.class)
  public void warmCache() {
    log.info("Starting cache warming...");

    // Load top 100 popular products
    List<Product> popularProducts = productRepository
      .findTop100ByOrderByViewCountDesc();

    popularProducts.forEach(product -> {
      String cacheKey = "product:" + product.getId();
      ProductDto dto = toDto(product);
      redisTemplate.opsForValue().set(cacheKey, dto, Duration.ofHours(1));
    });

    log.info("Cache warmed with {} products", popularProducts.size());
  }

  private ProductDto toDto(Product product) {
    return ProductDto.builder()
      .id(product.getId())
      .name(product.getName())
      .build();
  }
}

// ===== 5. Scheduled Cache Refresh (Background job) =====
@Component
@Slf4j
public class CacheRefreshScheduler {

  @Autowired
  private ProductRepository productRepository;

  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  @Scheduled(fixedRate = 30, timeUnit = TimeUnit.MINUTES) // Refresh m·ªói 30 ph√∫t
  public void refreshPopularProductsCache() {
    log.info("Refreshing popular products cache...");

    List<Product> products = productRepository.findTop100ByOrderByViewCountDesc();

    products.forEach(product -> {
      String cacheKey = "product:" + product.getId();
      ProductDto dto = toDto(product);
      redisTemplate.opsForValue().set(cacheKey, dto, Duration.ofHours(1));
    });

    log.info("Refreshed {} products in cache", products.size());
  }

  private ProductDto toDto(Product product) {
    return ProductDto.builder()
      .id(product.getId())
      .name(product.getName())
      .build();
  }
}

// ===== Dependencies: pom.xml =====
/*
<dependency>
  <groupId>org.redisson</groupId>
  <artifactId>redisson-spring-boot-starter</artifactId>
  <version>3.25.2</version>
</dependency>
*/

// ===== Config: RedissonConfig.java =====
@Configuration
public class RedissonConfig {

  @Bean
  public RedissonClient redissonClient() {
    Config config = new Config();
    config.useSingleServer()
      .setAddress("redis://localhost:6379")
      .setConnectionPoolSize(50)
      .setConnectionMinimumIdleSize(10);

    return Redisson.create(config);
  }
}
```

### ‚ùå C√°ch sai

```java
// ‚ùå SAI 1: Kh√¥ng x·ª≠ l√Ω cache stampede
@Cacheable(value = "products", key = "#id")
public ProductDto getProduct(Long id) {
  // 1000 concurrent requests ‚Üí 1000 DB queries khi cache expire
  return productRepository.findById(id).map(this::toDto).orElse(null);
}

// ‚ùå SAI 2: Lock local (kh√¥ng distributed)
private final Map<Long, Object> locks = new ConcurrentHashMap<>();

public ProductDto getProduct(Long id) {
  synchronized (locks.computeIfAbsent(id, k -> new Object())) {
    // ‚ùå Lock ch·ªâ work tr√™n 1 JVM
    // Multi-instance app ‚Üí m·ªói instance v·∫´n query DB
  }
}

// ‚ùå SAI 3: Lock timeout qu√° d√†i
RLock lock = redissonClient.getLock(lockKey);
lock.lock(60, TimeUnit.SECONDS); // ‚ùå 60s qu√° d√†i
// N·∫øu thread crash ‚Üí lock stuck 60s ‚Üí all requests fail

// ‚ùå SAI 4: Kh√¥ng double-check cache sau khi acquire lock
if (lock.tryLock()) {
  ProductDto product = queryDatabase(id); // ‚ùå Query tr·ª±c ti·∫øp
  cache.put(id, product);
  // N·∫øu 2 threads acquire lock tu·∫ßn t·ª± ‚Üí query DB 2 l·∫ßn
}
// FIX: Double-check cache tr∆∞·ªõc khi query

// ‚ùå SAI 5: Cache warming block app startup
@EventListener(ApplicationReadyEvent.class)
public void warmCache() {
  List<Product> products = productRepository.findAll(); // ‚ùå Load 1M records
  // App startup b·ªã block 10 ph√∫t
}
// FIX: Ch·ªâ load top N, ho·∫∑c async

// ‚ùå SAI 6: Singleflight kh√¥ng cleanup
private final Map<String, CompletableFuture<Object>> inflight = new ConcurrentHashMap<>();

public Object get(String key) {
  CompletableFuture<Object> future = inflight.computeIfAbsent(key, k ->
    CompletableFuture.supplyAsync(() -> queryDB(key))
    // ‚ùå Kh√¥ng remove kh·ªèi map sau khi done ‚Üí memory leak
  );
}

// ‚ùå SAI 7: XFetch v·ªõi BETA kh√¥ng ph√π h·ª£p
private static final double BETA = 10.0; // ‚ùå Qu√° l·ªõn
// ‚Üí Refresh qu√° s·ªõm ‚Üí cache hit rate th·∫•p ‚Üí DB overload
```

### Ph√°t hi·ªán

**Regex patterns:**
```regex
# @Cacheable kh√¥ng c√≥ lock/singleflight
@Cacheable(?!.*synchronized)(?!.*Lock)(?!.*tryLock)

# Lock local (synchronized) trong cache logic
synchronized.*\n.*cache

# Lock timeout > 30s
tryLock\(\d+,\s*(60|[1-9]\d{2,})

# Cache warming trong @PostConstruct/ApplicationReadyEvent
@(PostConstruct|EventListener).*\n.*public.*warmCache.*\n.*findAll\(\)
```

**Checklist:**
```java
// 1. High-traffic cache c√≥ singleflight/lock?
CompletableFuture<Object> future = inflightRequests.computeIfAbsent(...); // ‚úÖ

// 2. Distributed lock (kh√¥ng ph·∫£i local)?
RLock lock = redissonClient.getLock(lockKey); // ‚úÖ

// 3. Lock timeout h·ª£p l√Ω (< 10s)?
lock.tryLock(5, 10, TimeUnit.SECONDS); // ‚úÖ

// 4. Double-check cache sau acquire lock?
if (lock.tryLock()) {
  cached = redisTemplate.get(key); // Double-check
  if (cached != null) return cached;
  // Query DB
} // ‚úÖ

// 5. Cleanup inflight requests?
future.whenComplete((result, ex) -> inflightRequests.remove(key)); // ‚úÖ

// 6. Cache warming async + limit records?
@Async
public void warmCache() {
  List<Product> top100 = repo.findTop100(); // ‚úÖ
}

// 7. XFetch BETA tuning (0.5 - 2.0)?
private static final double BETA = 1.0; // ‚úÖ

// 8. Load test ƒë·ªÉ verify?
// JMeter: 1000 concurrent requests khi cache expire
// ‚Üí Ch·ªâ 1 DB query (singleflight work) ‚úÖ
```

---

## 10.06 - Serialization format ph√π h·ª£p (JSON vs Kryo vs Protobuf) üü°

### Metadata
- **ID:** `CACHE-006`
- **M·ª©c ƒë·ªô:** üü° N√äN C√ì
- **L√Ω do:** Serialization t·ªëi ∆∞u ‚Üí gi·∫£m 50-70% memory + network bandwidth
- **Trade-off:** Binary format (Kryo, Protobuf) ‚Üí kh√¥ng human-readable, compatibility risk

### T·∫°i sao?

**V·∫•n ƒë·ªÅ:**
- JSON: human-readable nh∆∞ng verbose (field names, whitespace)
- Example: `{"id":1,"name":"Product"}` vs binary `\x01\x07Product`
- 10K cache entries √ó 1KB JSON = 10MB vs 3MB binary ‚Üí 70% saving

**L·ª±a ch·ªçn serialization:**
1. **JSON (Jackson)**: Default, human-readable, debug d·ªÖ, t∆∞∆°ng th√≠ch t·ªët
2. **Kryo**: Binary, nhanh (2-10x), nh·ªè (50-70%), nh∆∞ng kh√¥ng version-safe
3. **Protobuf**: Binary, compact, version-safe, nh∆∞ng c·∫ßn schema (.proto file)
4. **FST**: Fast alternative to Java Serialization

**Khuy·∫øn ngh·ªã:**
- Dev/staging: JSON (debug d·ªÖ)
- Production: Kryo (performance) ho·∫∑c Protobuf (compatibility)

### ‚úÖ C√°ch ƒë√∫ng

```java
// ===== 1. JSON Serialization (Default) =====
@Configuration
public class JsonCacheConfig {

  @Bean
  public RedisCacheManager cacheManager(RedisConnectionFactory factory) {
    RedisCacheConfiguration config = RedisCacheConfiguration
      .defaultCacheConfig()
      .entryTtl(Duration.ofHours(1))
      .serializeKeysWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new StringRedisSerializer()))
      .serializeValuesWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new GenericJackson2JsonRedisSerializer()))
      .disableCachingNullValues();

    return RedisCacheManager.builder(factory)
      .cacheDefaults(config)
      .build();
  }
}

// ===== 2. Kryo Serialization (Fast + Compact) =====
@Configuration
public class KryoCacheConfig {

  @Bean
  public RedisCacheManager cacheManager(RedisConnectionFactory factory) {
    RedisCacheConfiguration config = RedisCacheConfiguration
      .defaultCacheConfig()
      .entryTtl(Duration.ofHours(1))
      .serializeKeysWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new StringRedisSerializer()))
      .serializeValuesWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new KryoRedisSerializer<>()))
      .disableCachingNullValues();

    return RedisCacheManager.builder(factory)
      .cacheDefaults(config)
      .build();
  }

  // Custom Kryo Serializer
  public static class KryoRedisSerializer<T> implements RedisSerializer<T> {

    private final ThreadLocal<Kryo> kryoThreadLocal = ThreadLocal.withInitial(() -> {
      Kryo kryo = new Kryo();
      kryo.setRegistrationRequired(false); // Auto-register classes
      kryo.setReferences(true); // Support circular references

      // Register common classes ƒë·ªÉ t·ªëi ∆∞u
      kryo.register(ArrayList.class);
      kryo.register(HashMap.class);
      kryo.register(HashSet.class);

      return kryo;
    });

    @Override
    public byte[] serialize(T value) throws SerializationException {
      if (value == null) {
        return null;
      }

      try (ByteArrayOutputStream baos = new ByteArrayOutputStream();
           Output output = new Output(baos)) {

        kryoThreadLocal.get().writeClassAndObject(output, value);
        output.flush();
        return baos.toByteArray();

      } catch (Exception e) {
        throw new SerializationException("Failed to serialize with Kryo", e);
      }
    }

    @Override
    @SuppressWarnings("unchecked")
    public T deserialize(byte[] bytes) throws SerializationException {
      if (bytes == null || bytes.length == 0) {
        return null;
      }

      try (Input input = new Input(new ByteArrayInputStream(bytes))) {
        return (T) kryoThreadLocal.get().readClassAndObject(input);
      } catch (Exception e) {
        throw new SerializationException("Failed to deserialize with Kryo", e);
      }
    }
  }
}

// ===== 3. Protobuf Serialization (Version-safe) =====
// File: product.proto
/*
syntax = "proto3";

package com.example.cache;

message ProductProto {
  int64 id = 1;
  string name = 2;
  double price = 3;
  string category = 4;
}
*/

@Configuration
public class ProtobufCacheConfig {

  @Bean
  public RedisCacheManager cacheManager(RedisConnectionFactory factory) {
    RedisCacheConfiguration config = RedisCacheConfiguration
      .defaultCacheConfig()
      .entryTtl(Duration.ofHours(1))
      .serializeKeysWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new StringRedisSerializer()))
      .serializeValuesWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new ProtobufRedisSerializer()))
      .disableCachingNullValues();

    return RedisCacheManager.builder(factory)
      .cacheDefaults(config)
      .build();
  }

  public static class ProtobufRedisSerializer implements RedisSerializer<Message> {

    @Override
    public byte[] serialize(Message message) throws SerializationException {
      if (message == null) {
        return null;
      }
      return message.toByteArray();
    }

    @Override
    public Message deserialize(byte[] bytes) throws SerializationException {
      if (bytes == null || bytes.length == 0) {
        return null;
      }

      try {
        // C·∫ßn bi·∫øt message type ƒë·ªÉ parse
        // Workaround: l∆∞u type name trong header ho·∫∑c d√πng Any
        return ProductProto.parseFrom(bytes);
      } catch (InvalidProtocolBufferException e) {
        throw new SerializationException("Failed to deserialize protobuf", e);
      }
    }
  }
}

// ===== 4. Hybrid: JSON cho dev, Kryo cho prod =====
@Configuration
public class HybridCacheConfig {

  @Value("${spring.profiles.active:dev}")
  private String activeProfile;

  @Bean
  public RedisCacheManager cacheManager(RedisConnectionFactory factory) {
    RedisSerializer<?> valueSerializer = "prod".equals(activeProfile)
      ? new KryoRedisSerializer<>()
      : new GenericJackson2JsonRedisSerializer();

    RedisCacheConfiguration config = RedisCacheConfiguration
      .defaultCacheConfig()
      .entryTtl(Duration.ofHours(1))
      .serializeKeysWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new StringRedisSerializer()))
      .serializeValuesWith(RedisSerializationContext.SerializationPair
        .fromSerializer(valueSerializer))
      .disableCachingNullValues();

    return RedisCacheManager.builder(factory)
      .cacheDefaults(config)
      .build();
  }
}

// ===== 5. Compression cho large objects =====
public static class CompressedRedisSerializer<T> implements RedisSerializer<T> {

  private final RedisSerializer<T> delegate;

  public CompressedRedisSerializer(RedisSerializer<T> delegate) {
    this.delegate = delegate;
  }

  @Override
  public byte[] serialize(T value) throws SerializationException {
    byte[] serialized = delegate.serialize(value);
    if (serialized == null || serialized.length < 1024) {
      return serialized; // Kh√¥ng compress n·∫øu < 1KB
    }

    try (ByteArrayOutputStream baos = new ByteArrayOutputStream();
         GZIPOutputStream gzip = new GZIPOutputStream(baos)) {

      gzip.write(serialized);
      gzip.finish();
      return baos.toByteArray();

    } catch (IOException e) {
      throw new SerializationException("Compression failed", e);
    }
  }

  @Override
  public T deserialize(byte[] bytes) throws SerializationException {
    if (bytes == null || bytes.length == 0) {
      return null;
    }

    try {
      // Detect GZIP magic number (1f 8b)
      if (bytes.length > 2 && bytes[0] == (byte) 0x1f && bytes[1] == (byte) 0x8b) {
        try (ByteArrayInputStream bais = new ByteArrayInputStream(bytes);
             GZIPInputStream gzip = new GZIPInputStream(bais);
             ByteArrayOutputStream baos = new ByteArrayOutputStream()) {

          byte[] buffer = new byte[4096];
          int len;
          while ((len = gzip.read(buffer)) > 0) {
            baos.write(buffer, 0, len);
          }
          bytes = baos.toByteArray();
        }
      }

      return delegate.deserialize(bytes);

    } catch (IOException e) {
      throw new SerializationException("Decompression failed", e);
    }
  }
}

// Usage
@Bean
public RedisCacheManager cacheManager(RedisConnectionFactory factory) {
  RedisSerializer<?> valueSerializer = new CompressedRedisSerializer<>(
    new GenericJackson2JsonRedisSerializer()
  );

  RedisCacheConfiguration config = RedisCacheConfiguration
    .defaultCacheConfig()
    .serializeValuesWith(RedisSerializationContext.SerializationPair
      .fromSerializer(valueSerializer));

  return RedisCacheManager.builder(factory)
    .cacheDefaults(config)
    .build();
}

// ===== 6. Benchmark serialization performance =====
@Component
public class SerializationBenchmark {

  @Test
  public void benchmarkSerializers() {
    ProductDto product = ProductDto.builder()
      .id(1L)
      .name("Test Product")
      .price(BigDecimal.valueOf(99.99))
      .build();

    List<RedisSerializer<ProductDto>> serializers = List.of(
      new GenericJackson2JsonRedisSerializer(),
      new KryoRedisSerializer<>()
    );

    serializers.forEach(serializer -> {
      long start = System.nanoTime();

      for (int i = 0; i < 10000; i++) {
        byte[] serialized = serializer.serialize(product);
        ProductDto deserialized = (ProductDto) serializer.deserialize(serialized);
      }

      long duration = System.nanoTime() - start;
      byte[] sample = serializer.serialize(product);

      System.out.printf("%s: %dms, size: %d bytes%n",
        serializer.getClass().getSimpleName(),
        duration / 1_000_000,
        sample.length
      );
    });
  }
}

// ===== Dependencies: pom.xml =====
/*
<!-- Kryo -->
<dependency>
  <groupId>com.esotericsoftware</groupId>
  <artifactId>kryo</artifactId>
  <version>5.5.0</version>
</dependency>

<!-- Protobuf -->
<dependency>
  <groupId>com.google.protobuf</groupId>
  <artifactId>protobuf-java</artifactId>
  <version>3.25.1</version>
</dependency>

<!-- FST -->
<dependency>
  <groupId>de.ruedigermoeller</groupId>
  <artifactId>fst</artifactId>
  <version>3.0.4</version>
</dependency>
*/
```

### ‚ùå C√°ch sai

```java
// ‚ùå SAI 1: D√πng Java default serialization
config.serializeValuesWith(RedisSerializationContext.SerializationPair
  .fromSerializer(new JdkSerializationRedisSerializer())
);
// ‚ùå Ch·∫≠m (10x so v·ªõi Kryo), l·ªõn (2-3x so v·ªõi JSON), security risk

// ‚ùå SAI 2: Kryo kh√¥ng config references
Kryo kryo = new Kryo();
kryo.setReferences(false); // ‚ùå Circular reference ‚Üí StackOverflowError

// ‚ùå SAI 3: Protobuf deserialize sai type
public Message deserialize(byte[] bytes) {
  return ProductProto.parseFrom(bytes); // ‚ùå Hardcode type
  // N·∫øu cache ch·ª©a UserProto ‚Üí parse fail
}
// FIX: L∆∞u type info trong metadata ho·∫∑c d√πng separate cache per type

// ‚ùå SAI 4: Compress m·ªçi object (k·ªÉ c·∫£ nh·ªè)
public byte[] serialize(Object value) {
  byte[] serialized = delegate.serialize(value);
  return compress(serialized); // ‚ùå Compress 10 bytes ‚Üí l√£ng ph√≠ CPU
}
// FIX: Ch·ªâ compress n·∫øu > 1KB

// ‚ùå SAI 5: JSON v·ªõi circular reference
@JsonBackReference // ‚ùå Qu√™n annotate
public class Product {
  private Category category;
}

public class Category {
  private List<Product> products; // Circular ref ‚Üí serialize fail
}

// ‚ùå SAI 6: Kryo thread-unsafe
private final Kryo kryo = new Kryo(); // ‚ùå Shared instance

public byte[] serialize(Object value) {
  // Thread 1 v√† Thread 2 c√πng d√πng kryo ‚Üí race condition
  return kryo.writeObjectOrNull(new Output(), value).toBytes();
}
// FIX: D√πng ThreadLocal<Kryo>

// ‚ùå SAI 7: Kh√¥ng benchmark tr∆∞·ªõc khi production
// Ch·ªçn Kryo v√¨ "nghe n√≥i nhanh" ‚Üí kh√¥ng test v·ªõi real data
// ‚Üí Production: Kryo slower than JSON (v√¨ data structure ph·ª©c t·∫°p)
```

### Ph√°t hi·ªán

**Regex patterns:**
```regex
# D√πng JdkSerializationRedisSerializer
JdkSerializationRedisSerializer

# Kryo kh√¥ng d√πng ThreadLocal
private.*Kryo\s+kryo(?!.*ThreadLocal)

# Compression kh√¥ng check size
compress\(serialized\)(?!.*length|size)

# Protobuf deserialize hardcode type
parseFrom\(bytes\)(?!.*switch|instanceof)
```

**Checklist:**
```java
// 1. KH√îNG d√πng JdkSerializationRedisSerializer?
// ‚ùå JdkSerializationRedisSerializer
// ‚úÖ GenericJackson2JsonRedisSerializer ho·∫∑c Kryo

// 2. Kryo d√πng ThreadLocal?
ThreadLocal<Kryo> kryoThreadLocal = ThreadLocal.withInitial(...); // ‚úÖ

// 3. Compression ch·ªâ cho large objects?
if (serialized.length > 1024) compress(...); // ‚úÖ

// 4. Profile-based serializer (JSON dev, Kryo prod)?
String profile = env.getActiveProfiles()[0];
RedisSerializer<?> serializer = "prod".equals(profile)
  ? new KryoRedisSerializer()
  : new GenericJackson2JsonRedisSerializer(); // ‚úÖ

// 5. Benchmark v·ªõi real data?
@Test
void benchmarkSerializers() {
  // Test v·ªõi 1000 ProductDto
  // Measure: serialize time, size, deserialize time
} // ‚úÖ

// 6. Circular reference handled?
@JsonManagedReference / @JsonBackReference // ‚úÖ
// ho·∫∑c kryo.setReferences(true)

// 7. Monitor serialization errors?
@ExceptionHandler(SerializationException.class)
public ResponseEntity<?> handleSerializationError(SerializationException e) {
  log.error("Serialization failed", e);
  return ResponseEntity.status(500).body("Cache error");
} // ‚úÖ
```

---

## 10.07 - Cache metrics (hit rate, miss rate, eviction) üü°

### Metadata
- **ID:** `CACHE-007`
- **M·ª©c ƒë·ªô:** üü° N√äN C√ì
- **L√Ω do:** Monitoring ‚Üí ph√°t hi·ªán cache ineffective, tune TTL, detect issues
- **Trade-off:** Metrics overhead (1-2% performance), storage cost

### T·∫°i sao?

**V·∫•n ƒë·ªÅ:**
- Cache hit rate th·∫•p (< 70%) ‚Üí TTL qu√° ng·∫Øn ho·∫∑c key strategy sai
- Eviction rate cao ‚Üí memory kh√¥ng ƒë·ªß ‚Üí c·∫ßn tƒÉng maxmemory
- Latency tƒÉng ƒë·ªôt ng·ªôt ‚Üí cache stampede ho·∫∑c Redis down

**Metrics c·∫ßn track:**
1. **Hit rate**: hits / (hits + misses) ‚Üí should be > 80%
2. **Miss rate**: 1 - hit rate
3. **Eviction rate**: entries evicted per second ‚Üí should be ~0
4. **Latency**: cache GET/SET time ‚Üí should be < 5ms
5. **Memory usage**: used memory / maxmemory ‚Üí should be < 80%
6. **Key count**: total keys per cache

### ‚úÖ C√°ch ƒë√∫ng

```java
// ===== 1. Micrometer Metrics v·ªõi Spring Boot Actuator =====
@Configuration
public class CacheMetricsConfig {

  @Bean
  public CacheManager cacheManager(
    RedisConnectionFactory factory,
    MeterRegistry meterRegistry
  ) {
    RedisCacheConfiguration config = RedisCacheConfiguration
      .defaultCacheConfig()
      .entryTtl(Duration.ofHours(1));

    RedisCacheManager cacheManager = RedisCacheManager.builder(factory)
      .cacheDefaults(config)
      .build();

    // Enable cache metrics
    CacheMetricsRegistrar.register(
      cacheManager,
      meterRegistry,
      "spring.cache" // Metric prefix
    );

    return cacheManager;
  }
}

// application.yml
/*
management:
  endpoints:
    web:
      exposure:
        include: health,metrics,prometheus
  metrics:
    export:
      prometheus:
        enabled: true
    tags:
      application: myapp
      environment: production
*/

// ===== 2. Custom Cache Statistics =====
@Component
@Slf4j
public class CacheStatistics {

  private final AtomicLong hits = new AtomicLong(0);
  private final AtomicLong misses = new AtomicLong(0);
  private final AtomicLong puts = new AtomicLong(0);
  private final AtomicLong evictions = new AtomicLong(0);

  @Autowired
  private MeterRegistry meterRegistry;

  @PostConstruct
  public void init() {
    // Register Gauges
    Gauge.builder("cache.hit.rate", this, CacheStatistics::getHitRate)
      .description("Cache hit rate")
      .register(meterRegistry);

    Gauge.builder("cache.miss.rate", this, CacheStatistics::getMissRate)
      .description("Cache miss rate")
      .register(meterRegistry);

    // Register Counters
    meterRegistry.counter("cache.hits", "result", "hit");
    meterRegistry.counter("cache.misses", "result", "miss");
    meterRegistry.counter("cache.puts", "operation", "put");
    meterRegistry.counter("cache.evictions", "operation", "evict");
  }

  public void recordHit() {
    hits.incrementAndGet();
    meterRegistry.counter("cache.hits").increment();
  }

  public void recordMiss() {
    misses.incrementAndGet();
    meterRegistry.counter("cache.misses").increment();
  }

  public void recordPut() {
    puts.incrementAndGet();
    meterRegistry.counter("cache.puts").increment();
  }

  public void recordEviction() {
    evictions.incrementAndGet();
    meterRegistry.counter("cache.evictions").increment();
  }

  public double getHitRate() {
    long totalRequests = hits.get() + misses.get();
    return totalRequests == 0 ? 0.0 : (double) hits.get() / totalRequests;
  }

  public double getMissRate() {
    return 1.0 - getHitRate();
  }

  @Scheduled(fixedRate = 60000) // Log m·ªói ph√∫t
  public void logStatistics() {
    log.info("Cache Statistics - Hit Rate: {:.2f}%, Misses: {}, Evictions: {}",
      getHitRate() * 100,
      misses.get(),
      evictions.get()
    );
  }

  public void reset() {
    hits.set(0);
    misses.set(0);
    puts.set(0);
    evictions.set(0);
  }
}

// ===== 3. Cache Aspect ƒë·ªÉ track metrics =====
@Aspect
@Component
@Slf4j
public class CacheMetricsAspect {

  @Autowired
  private CacheStatistics statistics;

  @Autowired
  private MeterRegistry meterRegistry;

  @Around("@annotation(cacheable)")
  public Object aroundCacheable(ProceedingJoinPoint pjp, Cacheable cacheable) throws Throwable {
    String cacheName = cacheable.value()[0];
    Timer.Sample sample = Timer.start(meterRegistry);

    try {
      Object result = pjp.proceed();

      if (result != null) {
        statistics.recordHit();
        sample.stop(Timer.builder("cache.get.time")
          .tag("cache", cacheName)
          .tag("result", "hit")
          .register(meterRegistry));
      } else {
        statistics.recordMiss();
        sample.stop(Timer.builder("cache.get.time")
          .tag("cache", cacheName)
          .tag("result", "miss")
          .register(meterRegistry));
      }

      return result;

    } catch (Exception e) {
      sample.stop(Timer.builder("cache.get.time")
        .tag("cache", cacheName)
        .tag("result", "error")
        .register(meterRegistry));
      throw e;
    }
  }

  @AfterReturning("@annotation(cacheEvict)")
  public void afterCacheEvict(CacheEvict cacheEvict) {
    statistics.recordEviction();
    meterRegistry.counter("cache.evict.count", "cache", cacheEvict.value()[0])
      .increment();
  }

  @AfterReturning("@annotation(cachePut)")
  public void afterCachePut(CachePut cachePut) {
    statistics.recordPut();
    meterRegistry.counter("cache.put.count", "cache", cachePut.value()[0])
      .increment();
  }
}

// ===== 4. Redis Metrics v·ªõi RedisTemplate =====
@Component
@Slf4j
public class RedisMetrics {

  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  @Autowired
  private MeterRegistry meterRegistry;

  @Scheduled(fixedRate = 30000) // Collect m·ªói 30s
  public void collectRedisMetrics() {
    RedisConnection connection = null;
    try {
      connection = redisTemplate.getConnectionFactory().getConnection();
      Properties info = connection.info();

      // Memory usage
      long usedMemory = Long.parseLong(info.getProperty("used_memory", "0"));
      long maxMemory = Long.parseLong(info.getProperty("maxmemory", "0"));

      Gauge.builder("redis.memory.used", () -> usedMemory)
        .description("Redis used memory in bytes")
        .register(meterRegistry);

      Gauge.builder("redis.memory.max", () -> maxMemory)
        .description("Redis max memory in bytes")
        .register(meterRegistry);

      // Hit rate (t·ª´ Redis INFO stats)
      long keyspaceHits = Long.parseLong(info.getProperty("keyspace_hits", "0"));
      long keyspaceMisses = Long.parseLong(info.getProperty("keyspace_misses", "0"));
      long totalRequests = keyspaceHits + keyspaceMisses;

      double redisHitRate = totalRequests == 0 ? 0.0 : (double) keyspaceHits / totalRequests;

      Gauge.builder("redis.hit.rate", () -> redisHitRate)
        .description("Redis keyspace hit rate")
        .register(meterRegistry);

      // Connected clients
      long connectedClients = Long.parseLong(info.getProperty("connected_clients", "0"));

      Gauge.builder("redis.clients.connected", () -> connectedClients)
        .description("Number of connected Redis clients")
        .register(meterRegistry);

      // Evicted keys
      long evictedKeys = Long.parseLong(info.getProperty("evicted_keys", "0"));

      meterRegistry.counter("redis.evicted.keys", "total", String.valueOf(evictedKeys));

      log.debug("Redis Metrics - Hit Rate: {:.2f}%, Memory: {}MB / {}MB",
        redisHitRate * 100,
        usedMemory / 1024 / 1024,
        maxMemory / 1024 / 1024
      );

    } catch (Exception e) {
      log.error("Failed to collect Redis metrics", e);
    } finally {
      if (connection != null) {
        connection.close();
      }
    }
  }

  public Map<String, Object> getRedisInfo() {
    RedisConnection connection = null;
    try {
      connection = redisTemplate.getConnectionFactory().getConnection();
      Properties info = connection.info();

      return Map.of(
        "usedMemory", info.getProperty("used_memory"),
        "maxMemory", info.getProperty("maxmemory"),
        "hitRate", calculateHitRate(info),
        "connectedClients", info.getProperty("connected_clients"),
        "evictedKeys", info.getProperty("evicted_keys")
      );

    } finally {
      if (connection != null) {
        connection.close();
      }
    }
  }

  private double calculateHitRate(Properties info) {
    long hits = Long.parseLong(info.getProperty("keyspace_hits", "0"));
    long misses = Long.parseLong(info.getProperty("keyspace_misses", "0"));
    long total = hits + misses;
    return total == 0 ? 0.0 : (double) hits / total;
  }
}

// ===== 5. Grafana Dashboard Queries (PromQL) =====
/*
# Cache Hit Rate
rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))

# Cache Miss Rate
rate(cache_misses_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))

# Cache GET Latency (p95)
histogram_quantile(0.95, rate(cache_get_time_bucket[5m]))

# Redis Memory Usage
redis_memory_used / redis_memory_max * 100

# Eviction Rate
rate(redis_evicted_keys[5m])

# Keys per Cache
redis_db_keys{db="0"}
*/

// ===== 6. Alert Rules (Prometheus) =====
/*
groups:
  - name: cache_alerts
    rules:
      - alert: CacheHitRateLow
        expr: cache_hit_rate < 0.7
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Cache hit rate below 70%"
          description: "Cache {{ $labels.cache }} hit rate is {{ $value }}"

      - alert: RedisMemoryHigh
        expr: redis_memory_used / redis_memory_max > 0.9
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Redis memory usage > 90%"

      - alert: CacheEvictionHigh
        expr: rate(redis_evicted_keys[5m]) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High cache eviction rate"
*/

// ===== 7. Health Check v·ªõi Cache =====
@Component
public class CacheHealthIndicator implements HealthIndicator {

  @Autowired
  private RedisTemplate<String, Object> redisTemplate;

  @Autowired
  private CacheStatistics statistics;

  @Override
  public Health health() {
    try {
      redisTemplate.getConnectionFactory().getConnection().ping();

      double hitRate = statistics.getHitRate();

      if (hitRate < 0.5) {
        return Health.down()
          .withDetail("hitRate", hitRate)
          .withDetail("status", "Hit rate too low")
          .build();
      }

      return Health.up()
        .withDetail("hitRate", hitRate)
        .withDetail("hits", statistics.hits.get())
        .withDetail("misses", statistics.misses.get())
        .build();

    } catch (Exception e) {
      return Health.down()
        .withException(e)
        .build();
    }
  }
}

// ===== 8. Admin Endpoint ƒë·ªÉ view metrics =====
@RestController
@RequestMapping("/api/admin/cache/metrics")
public class CacheMetricsController {

  @Autowired
  private CacheStatistics statistics;

  @Autowired
  private RedisMetrics redisMetrics;

  @GetMapping
  public ResponseEntity<Map<String, Object>> getMetrics() {
    Map<String, Object> metrics = Map.of(
      "application", Map.of(
        "hitRate", statistics.getHitRate(),
        "missRate", statistics.getMissRate(),
        "hits", statistics.hits.get(),
        "misses", statistics.misses.get(),
        "evictions", statistics.evictions.get()
      ),
      "redis", redisMetrics.getRedisInfo()
    );

    return ResponseEntity.ok(metrics);
  }

  @PostMapping("/reset")
  @PreAuthorize("hasRole('ADMIN')")
  public ResponseEntity<String> resetMetrics() {
    statistics.reset();
    return ResponseEntity.ok("Metrics reset");
  }
}
```

### ‚ùå C√°ch sai

```java
// ‚ùå SAI 1: Kh√¥ng track metrics
@Cacheable("products")
public ProductDto getProduct(Long id) {
  // Kh√¥ng bi·∫øt cache hit hay miss, kh√¥ng tune ƒë∆∞·ª£c
}

// ‚ùå SAI 2: Log m·ªói request (qu√° verbose)
@Around("@annotation(Cacheable)")
public Object aroundCache(ProceedingJoinPoint pjp) {
  log.info("Cache GET for key: {}", key); // ‚ùå 10K requests/s = 10K logs/s
  // FIX: D√πng metrics, log aggregate (1 ph√∫t 1 l·∫ßn)
}

// ‚ùå SAI 3: Metrics blocking I/O
@Scheduled(fixedRate = 1000)
public void collectMetrics() {
  RedisConnection conn = redis.getConnection();
  Properties info = conn.info(); // ‚ùå Blocking call m·ªói gi√¢y
  // FIX: Collect m·ªói 30-60s, d√πng async
}

// ‚ùå SAI 4: Kh√¥ng set alert
// Hit rate xu·ªëng 30% ‚Üí kh√¥ng ai bi·∫øt ‚Üí performance degradation
// FIX: Prometheus alert khi hit rate < 70%

// ‚ùå SAI 5: Metrics kh√¥ng tag
meterRegistry.counter("cache.hits").increment(); // ‚ùå Kh√¥ng bi·∫øt cache n√†o
// FIX: .counter("cache.hits", "cache", cacheName)

// ‚ùå SAI 6: Kh√¥ng monitor Redis INFO stats
// Ch·ªâ track app-level metrics ‚Üí kh√¥ng bi·∫øt Redis memory/eviction
// FIX: Collect redis.info() metrics

// ‚ùå SAI 7: Dashboard kh√¥ng c√≥
// C√≥ metrics nh∆∞ng kh√¥ng visualize ‚Üí kh√¥ng actionable
// FIX: Setup Grafana dashboard
```

### Ph√°t hi·ªán

**Regex patterns:**
```regex
# @Cacheable kh√¥ng c√≥ metrics tracking
@Cacheable(?!.*@Around).*\n.*public

# Scheduled metrics collection qu√° frequent
@Scheduled\(fixedRate\s*=\s*[1-9]\d{0,2}\) # < 1000ms

# Metrics kh√¥ng c√≥ tags
meterRegistry\.counter\("[^"]+"\)\.increment\(\)(?!.*,)
```

**Checklist:**
```java
// 1. Enable Spring Boot Actuator metrics?
management.endpoints.web.exposure.include=metrics,prometheus // ‚úÖ

// 2. Register cache metrics?
CacheMetricsRegistrar.register(cacheManager, meterRegistry); // ‚úÖ

// 3. Track hit/miss rate?
@Around("@annotation(Cacheable)")
public Object track(ProceedingJoinPoint pjp) {
  // Record hit/miss
} // ‚úÖ

// 4. Monitor Redis INFO stats?
@Scheduled(fixedRate = 30000)
public void collectRedisMetrics() { } // ‚úÖ

// 5. Metrics c√≥ tags (cache name)?
meterRegistry.counter("cache.hits", "cache", cacheName); // ‚úÖ

// 6. Grafana dashboard setup?
# PromQL queries for hit rate, latency, eviction // ‚úÖ

// 7. Alert rules configured?
# Prometheus alert: cache_hit_rate < 0.7 // ‚úÖ

// 8. Health check includes cache?
@Component
public class CacheHealthIndicator implements HealthIndicator { } // ‚úÖ
```

---

## 10.08 - Multi-level cache (L1 local + L2 Redis) khi c·∫ßn üü°

### Metadata
- **ID:** `CACHE-008`
- **M·ª©c ƒë·ªô:** üü° N√äN C√ì
- **L√Ω do:** L1 (Caffeine) < 1ms, L2 (Redis) ~5ms ‚Üí gi·∫£m 80% Redis calls
- **Trade-off:** Invalidation ph·ª©c t·∫°p, memory overhead, consistency risk

### T·∫°i sao?

**V·∫•n ƒë·ªÅ:**
- Redis network roundtrip: 3-10ms (LAN), 50-100ms (cross-region)
- High QPS (10K/s) ‚Üí 10K Redis calls ‚Üí network bottleneck
- Static data (categories, settings) query Redis kh√¥ng c·∫ßn thi·∫øt

**Gi·∫£i ph√°p: Multi-level cache**
- **L1 (local)**: Caffeine in-memory cache (< 1ms latency)
- **L2 (distributed)**: Redis cache (3-10ms latency)
- Read: L1 ‚Üí L2 ‚Üí DB
- Write: Invalidate L1 + L2

**Khi n√†o d√πng:**
- ‚úÖ Static/semi-static data (categories, configs)
- ‚úÖ High read QPS (> 1000/s)
- ‚úÖ Multi-instance app (d√πng Redis pub/sub ƒë·ªÉ sync L1)
- ‚ùå Real-time data (prices, inventory)
- ‚ùå Large objects (> 1MB) ‚Üí ch·ªâ d√πng L2

### ‚úÖ C√°ch ƒë√∫ng

```java
// ===== 1. Config Multi-level Cache =====
@Configuration
@EnableCaching
public class MultiLevelCacheConfig {

  @Bean
  public CacheManager cacheManager(
    RedisConnectionFactory redisConnectionFactory,
    RedisMessageListenerContainer listenerContainer
  ) {
    // L1: Caffeine local cache
    CaffeineCacheManager caffeineCacheManager = new CaffeineCacheManager();
    caffeineCacheManager.setCaffeine(Caffeine.newBuilder()
      .maximumSize(1000) // Max 1000 entries
      .expireAfterWrite(10, TimeUnit.MINUTES)
      .recordStats() // Enable metrics
    );

    // L2: Redis distributed cache
    RedisCacheConfiguration redisConfig = RedisCacheConfiguration
      .defaultCacheConfig()
      .entryTtl(Duration.ofHours(1))
      .serializeValuesWith(RedisSerializationContext.SerializationPair
        .fromSerializer(new GenericJackson2JsonRedisSerializer()));

    RedisCacheManager redisCacheManager = RedisCacheManager.builder(redisConnectionFactory)
      .cacheDefaults(redisConfig)
      .build();

    // Multi-level wrapper
    return new MultiLevelCacheManager(
      caffeineCacheManager,
      redisCacheManager,
      listenerContainer
    );
  }

  @Bean
  public RedisMessageListenerContainer redisMessageListenerContainer(
    RedisConnectionFactory connectionFactory
  ) {
    RedisMessageListenerContainer container = new RedisMessageListenerContainer();
    container.setConnectionFactory(connectionFactory);
    return container;
  }
}

// ===== 2. MultiLevelCacheManager Implementation =====
public class MultiLevelCacheManager implements CacheManager {

  private final CacheManager l1CacheManager; // Caffeine
  private final CacheManager l2CacheManager; // Redis
  private final RedisMessageListenerContainer listenerContainer;
  private final Map<String, MultiLevelCache> caches = new ConcurrentHashMap<>();

  public MultiLevelCacheManager(
    CacheManager l1,
    CacheManager l2,
    RedisMessageListenerContainer listenerContainer
  ) {
    this.l1CacheManager = l1;
    this.l2CacheManager = l2;
    this.listenerContainer = listenerContainer;
  }

  @Override
  public Cache getCache(String name) {
    return caches.computeIfAbsent(name, cacheName -> {
      Cache l1 = l1CacheManager.getCache(cacheName);
      Cache l2 = l2CacheManager.getCache(cacheName);
      return new MultiLevelCache(cacheName, l1, l2, listenerContainer);
    });
  }

  @Override
  public Collection<String> getCacheNames() {
    Set<String> names = new HashSet<>();
    names.addAll(l1CacheManager.getCacheNames());
    names.addAll(l2CacheManager.getCacheNames());
    return names;
  }
}

// ===== 3. MultiLevelCache Implementation =====
@Slf4j
public class MultiLevelCache implements Cache {

  private final String name;
  private final Cache l1Cache; // Caffeine
  private final Cache l2Cache; // Redis
  private final RedisMessageListenerContainer listenerContainer;
  private final String invalidationChannel;

  public MultiLevelCache(
    String name,
    Cache l1,
    Cache l2,
    RedisMessageListenerContainer listenerContainer
  ) {
    this.name = name;
    this.l1Cache = l1;
    this.l2Cache = l2;
    this.listenerContainer = listenerContainer;
    this.invalidationChannel = "cache:invalidate:" + name;

    // Subscribe to invalidation messages
    listenerContainer.addMessageListener(
      (message, pattern) -> {
        String key = new String(message.getBody());
        log.info("Received L1 invalidation for cache: {}, key: {}", name, key);
        if (l1Cache != null) {
          l1Cache.evict(key);
        }
      },
      new ChannelTopic(invalidationChannel)
    );
  }

  @Override
  public String getName() {
    return name;
  }

  @Override
  public Object getNativeCache() {
    return Map.of("l1", l1Cache, "l2", l2Cache);
  }

  @Override
  public ValueWrapper get(Object key) {
    // 1. Check L1
    ValueWrapper l1Value = l1Cache.get(key);
    if (l1Value != null) {
      log.debug("L1 cache HIT for key: {}", key);
      return l1Value;
    }

    // 2. Check L2
    ValueWrapper l2Value = l2Cache.get(key);
    if (l2Value != null) {
      log.debug("L2 cache HIT for key: {}, promoting to L1", key);
      // Promote to L1
      l1Cache.put(key, l2Value.get());
      return l2Value;
    }

    log.debug("Cache MISS (L1 + L2) for key: {}", key);
    return null;
  }

  @Override
  public <T> T get(Object key, Class<T> type) {
    ValueWrapper wrapper = get(key);
    return wrapper != null ? (T) wrapper.get() : null;
  }

  @Override
  public <T> T get(Object key, Callable<T> valueLoader) {
    ValueWrapper wrapper = get(key);
    if (wrapper != null) {
      return (T) wrapper.get();
    }

    // Load value
    try {
      T value = valueLoader.call();
      put(key, value);
      return value;
    } catch (Exception e) {
      throw new ValueRetrievalException(key, valueLoader, e);
    }
  }

  @Override
  public void put(Object key, Object value) {
    // Write to both levels
    l2Cache.put(key, value); // L2 first (persistent)
    l1Cache.put(key, value); // L1 second (fast)
    log.debug("Put to L1 + L2 cache, key: {}", key);
  }

  @Override
  public void evict(Object key) {
    // Evict from both levels
    l2Cache.evict(key);
    l1Cache.evict(key);

    // Broadcast invalidation to other instances
    publishInvalidation(key.toString());
    log.debug("Evicted from L1 + L2 cache, key: {}", key);
  }

  @Override
  public void clear() {
    l2Cache.clear();
    l1Cache.clear();
    publishInvalidation("*"); // Wildcard clear
    log.info("Cleared L1 + L2 cache: {}", name);
  }

  private void publishInvalidation(String key) {
    RedisConnection connection = null;
    try {
      connection = listenerContainer.getConnectionFactory().getConnection();
      connection.publish(
        invalidationChannel.getBytes(),
        key.getBytes()
      );
    } finally {
      if (connection != null) {
        connection.close();
      }
    }
  }
}

// ===== 4. Service Usage =====
@Service
public class ProductService {

  @Autowired
  private ProductRepository productRepository;

  // Multi-level cache t·ª± ƒë·ªông (qua CacheManager)
  @Cacheable(value = "products", key = "#id", unless = "#result == null")
  public ProductDto getProduct(Long id) {
    log.info("Querying DB for product: {}", id);
    return productRepository.findById(id)
      .map(this::toDto)
      .orElse(null);
  }

  @CacheEvict(value = "products", key = "#id")
  public ProductDto updateProduct(Long id, UpdateProductRequest request) {
    // Evict s·∫Ω x√≥a L1 + L2 + broadcast invalidation
    Product product = productRepository.findById(id).orElseThrow();
    product.setName(request.name());
    return toDto(productRepository.save(product));
  }

  private ProductDto toDto(Product product) {
    return ProductDto.builder()
      .id(product.getId())
      .name(product.getName())
      .build();
  }
}

// ===== 5. Advanced: Conditional L1 caching =====
@Service
public class ConditionalMultiLevelService {

  @Autowired
  private CacheManager cacheManager;

  public ProductDto getProduct(Long id, boolean useL1) {
    String cacheKey = "product:" + id;

    if (useL1) {
      // Use multi-level cache
      Cache cache = cacheManager.getCache("products");
      return cache.get(cacheKey, ProductDto.class);
    } else {
      // Bypass L1, only use L2
      MultiLevelCache mlCache = (MultiLevelCache) cacheManager.getCache("products");
      Cache l2Only = (Cache) ((Map<?, ?>) mlCache.getNativeCache()).get("l2");
      return l2Only.get(cacheKey, ProductDto.class);
    }
  }
}

// ===== 6. Metrics per Level =====
@Component
@Slf4j
public class MultiLevelCacheMetrics {

  @Autowired
  private CacheManager cacheManager;

  @Autowired
  private MeterRegistry meterRegistry;

  @Scheduled(fixedRate = 60000)
  public void reportMetrics() {
    cacheManager.getCacheNames().forEach(cacheName -> {
      Cache cache = cacheManager.getCache(cacheName);

      if (cache instanceof MultiLevelCache mlCache) {
        Map<String, Cache> nativeCaches = (Map<String, Cache>) mlCache.getNativeCache();

        // L1 stats (Caffeine)
        Cache l1 = nativeCaches.get("l1");
        if (l1.getNativeCache() instanceof com.github.benmanes.caffeine.cache.Cache caffeine) {
          com.github.benmanes.caffeine.cache.stats.CacheStats stats = caffeine.stats();

          meterRegistry.gauge("cache.l1.hit.rate", stats.hitRate());
          meterRegistry.gauge("cache.l1.miss.rate", stats.missRate());
          meterRegistry.gauge("cache.l1.eviction.count", stats.evictionCount());

          log.info("L1 Cache [{}] - Hit Rate: {:.2f}%, Evictions: {}",
            cacheName,
            stats.hitRate() * 100,
            stats.evictionCount()
          );
        }
      }
    });
  }
}

// ===== Dependencies: pom.xml =====
/*
<dependency>
  <groupId>com.github.ben-manes.caffeine</groupId>
  <artifactId>caffeine</artifactId>
  <version>3.1.8</version>
</dependency>
*/
```

### ‚ùå C√°ch sai

```java
// ‚ùå SAI 1: L1 kh√¥ng c√≥ TTL
Caffeine.newBuilder()
  .maximumSize(10000); // ‚ùå Kh√¥ng expire ‚Üí stale data m√£i m√£i
// FIX: Th√™m .expireAfterWrite(10, TimeUnit.MINUTES)

// ‚ùå SAI 2: Kh√¥ng invalidate L1 khi evict L2
@CacheEvict(value = "products", key = "#id")
public void updateProduct(Long id) {
  // ‚ùå Ch·ªâ evict Redis, L1 v·∫´n gi·ªØ data c≈©
}
// FIX: MultiLevelCache.evict() ph·∫£i evict c·∫£ 2

// ‚ùå SAI 3: Kh√¥ng broadcast invalidation (multi-instance)
public void evict(Object key) {
  l1Cache.evict(key);
  l2Cache.evict(key);
  // ‚ùå Instance 2, 3 v·∫´n gi·ªØ data c≈© trong L1
}
// FIX: Publish Redis message ƒë·ªÉ sync

// ‚ùå SAI 4: L1 size qu√° l·ªõn
Caffeine.newBuilder()
  .maximumSize(1_000_000); // ‚ùå 1M entries ‚Üí OutOfMemoryError
// FIX: Size < 10K cho most apps

// ‚ùå SAI 5: Cache large objects trong L1
@Cacheable("reports") // L1 + L2
public byte[] generateReport(Long id) {
  return new byte[50 * 1024 * 1024]; // ‚ùå 50MB per entry
}
// FIX: Large objects ch·ªâ cache L2 (Redis), skip L1

// ‚ùå SAI 6: Kh√¥ng monitor L1 hit rate
// L1 hit rate th·∫•p (< 50%) ‚Üí kh√¥ng c·∫ßn L1, t·ªën memory v√¥ √≠ch
// FIX: Track metrics, disable L1 n·∫øu kh√¥ng effective

// ‚ùå SAI 7: L1 + L2 c√πng TTL
L1: expireAfterWrite(1, TimeUnit.HOURS)
L2: entryTtl(Duration.ofHours(1))
// ‚ùå L1 expire ‚Üí L2 promote ‚Üí L1 expire l·∫°i ‚Üí thrashing
// FIX: L1 TTL ng·∫Øn h∆°n L2 (L1: 10min, L2: 1 hour)
```

### Ph√°t hi·ªán

**Regex patterns:**
```regex
# Caffeine kh√¥ng c√≥ expire
Caffeine\.newBuilder\(\).*maximumSize(?!.*expireAfter)

# @CacheEvict kh√¥ng broadcast
@CacheEvict.*\n.*public.*update(?!.*publish|broadcast)

# L1 size qu√° l·ªõn
maximumSize\(([1-9]\d{5,})\) # > 100K

# Large object cache trong L1
@Cacheable.*\n.*public\s+byte\[\]
```

**Checklist:**
```java
// 1. L1 c√≥ TTL?
Caffeine.newBuilder()
  .expireAfterWrite(10, TimeUnit.MINUTES) // ‚úÖ

// 2. L1 size h·ª£p l√Ω (< 10K)?
.maximumSize(1000) // ‚úÖ

// 3. Evict c·∫£ L1 + L2?
@Override
public void evict(Object key) {
  l1Cache.evict(key);
  l2Cache.evict(key);
} // ‚úÖ

// 4. Broadcast invalidation?
publishInvalidation(key.toString()); // ‚úÖ

// 5. L1 TTL < L2 TTL?
L1: 10 minutes, L2: 1 hour // ‚úÖ

// 6. Monitor L1 hit rate?
@Scheduled
public void reportL1Metrics() {
  CacheStats stats = caffeine.stats();
  log.info("L1 Hit Rate: {}", stats.hitRate());
} // ‚úÖ

// 7. Large objects skip L1?
if (size > 1MB) {
  l2Cache.put(key, value); // Only L2
} else {
  put(key, value); // L1 + L2
} // ‚úÖ

// 8. Test multi-instance invalidation?
@Test
void testCrossInstanceInvalidation() {
  // Instance 1: put(key, value)
  // Instance 2: evict(key)
  // Instance 1: get(key) ‚Üí null ‚úÖ
}
```

---

**End of Domain 10: Caching**
